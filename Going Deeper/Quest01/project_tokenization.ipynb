{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 환경 세팅 및 데이터 다운로드"
      ],
      "metadata": {
        "id": "JAwty53Qyc4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/AIFFEL/Deep_Dive/work/make_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9K25fDPuyF6",
        "outputId": "448d630c-f136-4e80-c497-bb2c2a90f718"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/AIFFEL/Deep_Dive/work/make_vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgMyNg22t5D3",
        "outputId": "909cb508-1244-4cdc-969e-8553a89f3f15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.12/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from konlpy) (1.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.12/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: soynlp in /usr/local/lib/python3.12/dist-packages (0.0.493)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.12/dist-packages (from soynlp) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.12/dist-packages (from soynlp) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from soynlp) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from soynlp) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.20.0->soynlp) (3.6.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "openjdk-17-jdk is already the newest version (17.0.16+8~us1-0ubuntu1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "fatal: destination path 'Mecab-ko-for-Google-Colab' already exists and is not an empty directory.\n",
            "/content/drive/MyDrive/AIFFEL/Deep_Dive/work/make_vocab/Mecab-ko-for-Google-Colab\n",
            "Installing konlpy.....\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.12/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from konlpy) (1.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.12/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
            "Done\n",
            "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
            "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "--2025-11-13 07:54:17--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.142.25, 104.192.142.26, 104.192.142.24, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.142.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNK34QW3B3&Signature=ysmfLAbb%2F%2FffHrZ%2FB5HFkvdhShU%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEID%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIDmCTiLzsGt4i1mbcpl0DeC3Y7xkM7AjToOCNu31NkJCAiEA%2BPQmhQhvyWNMIsu7StGzHjb3RQtfR%2BENqShySrFeX%2FAqpwIISRAAGgw5ODQ1MjUxMDExNDYiDLlMEAArayRLLa6rPyqEAqwgdNEGg7xK8Urmu2uXAbVZLkzNChx6KwbS7rX3IFwkaHAtWmSh4n8Okdgy0lHql1P0PqyUEsNVBt%2FcQdxoqRwW%2BQg%2Fn9VXme66QtU4wh34iHo92C4Src9GpaDgDzyNTWr6bNNvGYomE5LKrZy0m381XoLi5BSw8%2Bsqn6hP%2FVnT400YokfFcHYCGSS1eZkf9iOZvm90W%2BQ0XeVNJAH6anENQIWxvZbkhIT9kWdyBTlvkcSqGwfcLpyriAgaI%2FbkO0rbgPKeDD1hEPs915curM7LCbvYoEqZ5kfsq%2Bzs%2BuqjfAmRjfTQn5XX8dCIcE2B7jAZrrWqFFew0B3djxP4EBWf6aAaMKqb1sgGOp0B0U%2FaWZxC4SpiAGfSIEl21XTvCSZIsw0WOyMDQn9qsBREhpwbQ8HNALK%2B4E%2B55TAvD2pw6E7ZzsJ15k7lxkdZq4OMwXCLIcNPQOpey4AljzlmJ980fkB4bjTEVLKtdvTTFEC43cUYoC66fmnKAoMThK4kKzRtO%2FoPIbzkZcFWOJ6PSMvk7f7PoC5vQg%2FFWHaid%2FhxBn0tCrcFwn8yrg%3D%3D&Expires=1763022002 [following]\n",
            "--2025-11-13 07:54:18--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNK34QW3B3&Signature=ysmfLAbb%2F%2FffHrZ%2FB5HFkvdhShU%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEID%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIDmCTiLzsGt4i1mbcpl0DeC3Y7xkM7AjToOCNu31NkJCAiEA%2BPQmhQhvyWNMIsu7StGzHjb3RQtfR%2BENqShySrFeX%2FAqpwIISRAAGgw5ODQ1MjUxMDExNDYiDLlMEAArayRLLa6rPyqEAqwgdNEGg7xK8Urmu2uXAbVZLkzNChx6KwbS7rX3IFwkaHAtWmSh4n8Okdgy0lHql1P0PqyUEsNVBt%2FcQdxoqRwW%2BQg%2Fn9VXme66QtU4wh34iHo92C4Src9GpaDgDzyNTWr6bNNvGYomE5LKrZy0m381XoLi5BSw8%2Bsqn6hP%2FVnT400YokfFcHYCGSS1eZkf9iOZvm90W%2BQ0XeVNJAH6anENQIWxvZbkhIT9kWdyBTlvkcSqGwfcLpyriAgaI%2FbkO0rbgPKeDD1hEPs915curM7LCbvYoEqZ5kfsq%2Bzs%2BuqjfAmRjfTQn5XX8dCIcE2B7jAZrrWqFFew0B3djxP4EBWf6aAaMKqb1sgGOp0B0U%2FaWZxC4SpiAGfSIEl21XTvCSZIsw0WOyMDQn9qsBREhpwbQ8HNALK%2B4E%2B55TAvD2pw6E7ZzsJ15k7lxkdZq4OMwXCLIcNPQOpey4AljzlmJ980fkB4bjTEVLKtdvTTFEC43cUYoC66fmnKAoMThK4kKzRtO%2FoPIbzkZcFWOJ6PSMvk7f7PoC5vQg%2FFWHaid%2FhxBn0tCrcFwn8yrg%3D%3D&Expires=1763022002\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 16.15.219.12, 52.216.79.4, 54.231.202.9, ...\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|16.15.219.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz.1’\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  4.41MB/s    in 0.3s    \n",
            "\n",
            "2025-11-13 07:54:18 (4.41 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz.1’ saved [1414979/1414979]\n",
            "\n",
            "Done\n",
            "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-0.996-ko-0.9.2.......\n",
            "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
            "configure\n",
            "make\n",
            "make check\n",
            "^C\n",
            "/content/drive/MyDrive/AIFFEL/Deep_Dive/work/make_vocab\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy\n",
        "!pip install sentencepiece\n",
        "!pip install soynlp\n",
        "!apt-get install -y openjdk-17-jdk\n",
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd ./Mecab-ko-for-Google-Colab/\n",
        "!bash install_mecab-ko_on_colab_light_220429.sh\n",
        "%cd /content/drive/MyDrive/AIFFEL/Deep_Dive/work/make_vocab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] += os.pathsep + os.path.join(os.environ[\"JAVA_HOME\"], \"bin\")\n"
      ],
      "metadata": {
        "id": "9mmTMa6qmvOw"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "6vjJ2LVNu-AG"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 데이터 분석"
      ],
      "metadata": {
        "id": "XJGAnBlXyZEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/AIFFEL/work/sentiment_classification/data/\"\n",
        "raw_train = pd.read_table(base_path + 'ratings_train.txt')\n",
        "raw_test = pd.read_table(base_path + 'ratings_test.txt')\n",
        "\n",
        "raw_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "3_XvxdUHvMtG",
        "outputId": "192c0ad0-9a85-4852-8b45-d195b04e6533"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                                           document  label\n",
              "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
              "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
              "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5506e1eb-9e8f-41d4-ae00-233bd9f4030e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5506e1eb-9e8f-41d4-ae00-233bd9f4030e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5506e1eb-9e8f-41d4-ae00-233bd9f4030e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5506e1eb-9e8f-41d4-ae00-233bd9f4030e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ab885101-8edd-4e7e-9c4f-93b869424fca\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ab885101-8edd-4e7e-9c4f-93b869424fca')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ab885101-8edd-4e7e-9c4f-93b869424fca button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "raw_train"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "3vGNf8lmLJ0N",
        "outputId": "38cd7831-9411-4036-8501-306b2954935a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                           document  label\n",
              "0  6270596                                                굳 ㅋ      1\n",
              "1  9274899                               GDNTOPCLASSINTHECLUB      0\n",
              "2  8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
              "3  6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
              "4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9aa391bd-2b44-4adf-a4fd-4d51d54ad727\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6270596</td>\n",
              "      <td>굳 ㅋ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9274899</td>\n",
              "      <td>GDNTOPCLASSINTHECLUB</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8544678</td>\n",
              "      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6825595</td>\n",
              "      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6723715</td>\n",
              "      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9aa391bd-2b44-4adf-a4fd-4d51d54ad727')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9aa391bd-2b44-4adf-a4fd-4d51d54ad727 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9aa391bd-2b44-4adf-a4fd-4d51d54ad727');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a99925fc-75d7-4758-a5db-c47ae5c6aa50\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a99925fc-75d7-4758-a5db-c47ae5c6aa50')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a99925fc-75d7-4758-a5db-c47ae5c6aa50 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "raw_test",
              "summary": "{\n  \"name\": \"raw_test\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2936823,\n        \"min\": 601,\n        \"max\": 10278090,\n        \"num_unique_values\": 50000,\n        \"samples\": [\n          9147749,\n          498994,\n          7415980\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"document\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49157,\n        \"samples\": [\n          \"\\uc18c\\ub9ac\\uc9c0\\ub974\\uc9c0\\ub9c8\\ub77c!!\\uaf2d \\uacf5\\ud3ec\\uc601\\ud654\\uc5d0 \\uc790\\uc2e0\\uc5c6\\ub294 \\uac83\\ub4e4\\uc774 \\uc18c\\ub9ac\\ub9cc \\uc9c0\\ub974\\ub354\\ub77c~\\ud3ec\\uc2a4\\ud130\\uac00 \\ub354\\ubb34\\uc12d\\ub2e4\",\n          \"\\ud734...\\ucc39\\ucc39\\ud558\\ub124..\",\n          \"\\ub300\\ubc15~~~!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 결측치 처리"
      ],
      "metadata": {
        "id": "BddXBEDeINRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train 결측치 처리 전\")\n",
        "print(raw_train.isnull().sum())\n",
        "print(raw_train[raw_train.isnull().any(axis=1)].head())\n",
        "train_data = raw_train\n",
        "train_data['document'] = train_data['document'].fillna('')\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"train 결측치 처리 후\")\n",
        "print(train_data.isnull().sum())\n",
        "print(train_data[train_data.isnull().any(axis=1)].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XWOKV1CI1n1",
        "outputId": "b43e7165-a9d0-4006-db73-14b5f88f524c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 결측치 처리 전\n",
            "id          0\n",
            "document    5\n",
            "label       0\n",
            "dtype: int64\n",
            "             id document  label\n",
            "25857   2172111      NaN      1\n",
            "55737   6369843      NaN      1\n",
            "110014  1034280      NaN      0\n",
            "126782  5942978      NaN      0\n",
            "140721  1034283      NaN      0\n",
            "==================================================\n",
            "train 결측치 처리 후\n",
            "id          0\n",
            "document    0\n",
            "label       0\n",
            "dtype: int64\n",
            "Empty DataFrame\n",
            "Columns: [id, document, label]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"test 결측치 처리 전\")\n",
        "print(raw_test.isnull().sum())\n",
        "print(raw_test[raw_train.isnull().any(axis=1)].head())\n",
        "test_data = raw_test\n",
        "test_data['document'] = test_data['document'].fillna('')\n",
        "\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"test 결측치 처리 후\")\n",
        "print(test_data.isnull().sum())\n",
        "print(test_data[train_data.isnull().any(axis=1)].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6UDFiu3KynV",
        "outputId": "61769da2-2ed2-4e20-8a38-86d8870072ed"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test 결측치 처리 전\n",
            "id          0\n",
            "document    3\n",
            "label       0\n",
            "dtype: int64\n",
            "Empty DataFrame\n",
            "Columns: [id, document, label]\n",
            "Index: []\n",
            "==================================================\n",
            "test 결측치 처리 후\n",
            "id          0\n",
            "document    0\n",
            "label       0\n",
            "dtype: int64\n",
            "Empty DataFrame\n",
            "Columns: [id, document, label]\n",
            "Index: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2986361469.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  print(raw_test[raw_train.isnull().any(axis=1)].head())\n",
            "/tmp/ipython-input-2986361469.py:11: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  print(test_data[train_data.isnull().any(axis=1)].head())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리"
      ],
      "metadata": {
        "id": "nKaAZDVNIQmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# 불용어 리스트\n",
        "STOPWORDS = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
        "\n",
        "# 맞춤법 변형 사전\n",
        "SPELLING_DICT = {\n",
        "    '굳': ['궅', '굳', '굿'],\n",
        "    '미쳤': ['미첫', '미쳣', '미첬', '미쳤', 'ㅁㅊ'],\n",
        "    '괜찮': ['괜찮', '괜춘', '괜찬', 'ㄱㅊ', '갠찬', '갠찮', '괸찬', '괸찮'],\n",
        "    '봤': ['봣'],\n",
        "    '겠': ['겟']\n",
        "}\n",
        "\n",
        "# 텍스트 컬럼명\n",
        "TEXT_COL = \"document\"\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    텍스트 전처리 함수\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    text = str(text)\n",
        "\n",
        "    # 1. 반복되는 문장 부호 제거 (2개 이상 → 1개)\n",
        "    text = re.sub(r'([.!?…;])\\1+', r'\\1', text)\n",
        "\n",
        "    # 2. 맞춤법 변형 통일\n",
        "    for correct, variations in SPELLING_DICT.items():\n",
        "        for variant in variations:\n",
        "            text = text.replace(variant, correct)\n",
        "\n",
        "    # 3. 반복 문자 제거 (3번 이상 반복 → 2번)\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "\n",
        "    # 4. 자음/모음 단독 제거 (완성형 한글 필터링 전에 먼저 제거)\n",
        "    # 한글 자음: ㄱ-ㅎ, 한글 모음: ㅏ-ㅣ\n",
        "    text = re.sub(r'[ㄱ-ㅎㅏ-ㅣ]+', ' ', text)\n",
        "\n",
        "    # 5. 영어/숫자/한글/문장부호만 남기고 모두 삭제\n",
        "    # 이모지, 이모티콘, 특수문자 자동 제거\n",
        "    text = re.sub(r'[^가-힣a-zA-Z0-9\\s.!?,]', ' ', text)\n",
        "\n",
        "    # 6. 문장부호 앞뒤로 공백 추가\n",
        "    text = re.sub(r'([.!?,])', r' \\1 ', text)\n",
        "\n",
        "    # 7. 영어 소문자 변환\n",
        "    text = text.lower()\n",
        "\n",
        "    # 9. 불용어 제거 (조사가 붙은 경우도 처리)\n",
        "    words = text.split()\n",
        "    # 디버깅: 불용어 제거 전후 비교\n",
        "    # print(f\"불용어 제거 전: {words}\")\n",
        "\n",
        "    # 단어 끝에 불용어가 붙어있으면 제거\n",
        "    filtered_words = []\n",
        "    for word in words:\n",
        "        # 단어 전체가 불용어인 경우\n",
        "        if word in STOPWORDS:\n",
        "            continue\n",
        "        # 단어 끝에서 불용어 제거 (가장 긴 것부터 체크)\n",
        "        # 예: \"학교에서\" -> \"학교\", \"영화는\" -> \"영화\"\n",
        "        found = False\n",
        "        for stopword in sorted(STOPWORDS, key=len, reverse=True):\n",
        "            if len(word) > len(stopword) and word.endswith(stopword):\n",
        "                cleaned = word[:-len(stopword)]\n",
        "                # 남은 부분이 2글자 이상일 때만 제거\n",
        "                # 굳이 -> 굳, 같이 -> 같  이렇게 바뀌어서 길이 제한 추가했습니다.\n",
        "                # \"굳이\" -> \"굳\" (X), \"영화는\" -> \"영화\" (O)\n",
        "                if len(cleaned) >= 2:\n",
        "                    filtered_words.append(cleaned)\n",
        "                    found = True\n",
        "                    break\n",
        "        if not found:\n",
        "            filtered_words.append(word)\n",
        "\n",
        "    text = ' '.join(filtered_words)\n",
        "\n",
        "    # 10. 연속 공백을 하나의 공백으로 교체\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # 11. 앞뒤 공백 제거\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocess_dataframe(df, text_col=TEXT_COL):\n",
        "    \"\"\"\n",
        "    데이터프레임 전처리 함수\n",
        "    \"\"\"\n",
        "    print(f\"전처리 전 데이터 크기: {len(df)}\")\n",
        "\n",
        "    # 1. 결측치 제거\n",
        "    df = df.dropna(subset=[text_col])\n",
        "    print(f\"결측치 제거 후: {len(df)}\")\n",
        "\n",
        "    # 2. 텍스트 전처리 적용\n",
        "    df[text_col] = df[text_col].apply(preprocess_text)\n",
        "\n",
        "    # 3. 전처리 후 빈 문자열 제거\n",
        "    df = df[df[text_col].str.strip() != '']\n",
        "    print(f\"빈 문자열 제거 후: {len(df)}\")\n",
        "\n",
        "    # 4. 중복 행 제거\n",
        "    df = df.drop_duplicates(subset=[text_col])\n",
        "    print(f\"중복 제거 후: {len(df)}\")\n",
        "\n",
        "    # 인덱스 재설정\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# 샘플 텍스트로 테스트\n",
        "sample_texts = [\n",
        "    \"어제 본 영화 진짜 재밌었음!!! 또 보고 싶어 😂\",\n",
        "    \"나는 오늘 아침에 학교에 갔다. 근데 너무 졸렸음ㅋㅋㅋㅋ\",\n",
        "    \"밥은 먹었니?? 아직이야... 점심에 같이 먹자!!!\",\n",
        "    \"메캅 형태소 분석은 한국어 처리에서 많이 사용돼 👍\",\n",
        "    \"파이썬으로 토큰 빈도와 품사 분포를 시각화해 보자!!!\",\n",
        "    \"요즘 코사인 유사도 기반 벡터 검색으로 RG 구축을 많이 해!!\",\n",
        "    \"에이전트는 외부 도구를 호출해 작업을 자동화할 수 있어. 굳!\",\n",
        "    \"이 영화 진짜 미쳣다!!! 너무 재밌음ㅋㅋㅋㅋ\",\n",
        "    \"배우 연기 굳이 훌륭했음, 스토리는 봣지만...\",\n",
        "    \"이건 ㄱㅊ 영화네, 굿굿!\"\n",
        "]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"샘플 텍스트 전처리 결과\")\n",
        "print(\"=\" * 80)\n",
        "for i, text in enumerate(sample_texts, 1):\n",
        "    processed = preprocess_text(text)\n",
        "    print(f\"\\n[{i}] 원본: {text}\")\n",
        "    print(f\"    결과: {processed}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"실제 데이터 적용 예시\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "train_data = preprocess_dataframe(train_data.copy())\n",
        "test_data = preprocess_dataframe(test_data.copy())\n",
        "\n",
        "print(\"\\n전처리 완료!\")\n",
        "print(f\"Train 데이터: {len(train_data)}개\")\n",
        "print(f\"Test 데이터: {len(test_data)}개\")\n",
        "\n",
        "print(\"\\n전처리 결과 샘플:\")\n",
        "print(train_data.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9_63rSuvvpN",
        "outputId": "43afd176-1da1-44fa-cf03-a6dc2430deb1"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "샘플 텍스트 전처리 결과\n",
            "================================================================================\n",
            "\n",
            "[1] 원본: 어제 본 영화 진짜 재밌었음!!! 또 보고 싶어 😂\n",
            "    결과: 어제 본 영화 진짜 재밌었음 ! 또 보고 싶어\n",
            "\n",
            "[2] 원본: 나는 오늘 아침에 학교에 갔다. 근데 너무 졸렸음ㅋㅋㅋㅋ\n",
            "    결과: 나는 오늘 아침 학교 갔다 . 근데 너무 졸렸음\n",
            "\n",
            "[3] 원본: 밥은 먹었니?? 아직이야... 점심에 같이 먹자!!!\n",
            "    결과: 밥은 먹었니 ? 아직이야 . 점심 같이 먹자 !\n",
            "\n",
            "[4] 원본: 메캅 형태소 분석은 한국어 처리에서 많이 사용돼 👍\n",
            "    결과: 메캅 형태소 분석 한국어 처리에서 많이 사용돼\n",
            "\n",
            "[5] 원본: 파이썬으로 토큰 빈도와 품사 분포를 시각화해 보자!!!\n",
            "    결과: 파이썬 토큰 빈도 품사 분포 시각화해 보자 !\n",
            "\n",
            "[6] 원본: 요즘 코사인 유사도 기반 벡터 검색으로 RG 구축을 많이 해!!\n",
            "    결과: 요즘 코사인 유사 기반 벡터 검색 rg 구축을 많이 해 !\n",
            "\n",
            "[7] 원본: 에이전트는 외부 도구를 호출해 작업을 자동화할 수 있어. 굳!\n",
            "    결과: 에이전트 외부 도구 호출해 작업을 자동화할 수 있어 . 굳 !\n",
            "\n",
            "[8] 원본: 이 영화 진짜 미쳣다!!! 너무 재밌음ㅋㅋㅋㅋ\n",
            "    결과: 영화 진짜 미쳤다 ! 너무 재밌음\n",
            "\n",
            "[9] 원본: 배우 연기 굳이 훌륭했음, 스토리는 봣지만...\n",
            "    결과: 배우 연기 굳이 훌륭했음 , 스토리 봤지만 .\n",
            "\n",
            "[10] 원본: 이건 ㄱㅊ 영화네, 굿굿!\n",
            "    결과: 이건 괜찮 영화네 , 굳굳 !\n",
            "\n",
            "================================================================================\n",
            "실제 데이터 적용 예시\n",
            "================================================================================\n",
            "전처리 전 데이터 크기: 150000\n",
            "결측치 제거 후: 150000\n",
            "빈 문자열 제거 후: 149607\n",
            "중복 제거 후: 144478\n",
            "전처리 전 데이터 크기: 50000\n",
            "결측치 제거 후: 50000\n",
            "빈 문자열 제거 후: 49843\n",
            "중복 제거 후: 48700\n",
            "\n",
            "전처리 완료!\n",
            "Train 데이터: 144478개\n",
            "Test 데이터: 48700개\n",
            "\n",
            "전처리 결과 샘플:\n",
            "         id                                           document  label\n",
            "0   9976970                                아 더빙 . 진짜 짜증나네요 목소리      0\n",
            "1   3819312                   흠 . 포스터보고 초딩영화줄 . 오버연기조차 가볍지 않구나      1\n",
            "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
            "3   9045019                      교도소 이야기구먼 . 솔직히 재미 없다 . 평점 조정      0\n",
            "4   6483659  사이몬페그 익살스런 연기 돋보였던 영화 ! 스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n",
            "5   5403919        막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화 . . 별반개 아까움 .      0\n",
            "6   7797314                              원작 긴장감을 제대로 살려내지못했다 .      0\n",
            "7   9443947  별 반개 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지 . 정말 발로해 그것보단 ...      0\n",
            "8   7156791                                액션 없는데 재미 있는 몇안되 영화      1\n",
            "9   5912145     왜케 평점 낮은건데 ? 꽤 볼만한데 . 헐리우드식 화려함에만 너무 길들여져 있나 ?      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 시각화 및 통계"
      ],
      "metadata": {
        "id": "WkLAJA8dIVq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_len = 999\n",
        "max_len = 0\n",
        "sum_len = 0\n",
        "thr_len = []\n",
        "percent = [50, 60, 70, 80, 90, 95, 99]\n",
        "len_list = []\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "    sen = train_data['document'][i]\n",
        "    length = len(train_data['document'][i])\n",
        "    if min_len > length: min_len = length\n",
        "    if max_len < length: max_len = length\n",
        "    sum_len += length\n",
        "    len_list.append(length)\n",
        "\n",
        "len_list.sort()\n",
        "\n",
        "# XX% coverage -> sentence_lenght 사용해서 다시 진행 필요\n",
        "threshold = []\n",
        "for perc in percent:\n",
        "    threshold.append(sum_len * (perc/100))\n",
        "\n",
        "for thr in threshold:\n",
        "    thr_sum = 0\n",
        "    for length in len_list:\n",
        "        thr_sum += length\n",
        "        if thr_sum > thr:\n",
        "            thr_len.append(length)\n",
        "            break\n",
        "\n",
        "coverage_dict = dict(zip(percent, thr_len))\n",
        "\n",
        "print(\"문장의 최단 길이:\", min_len)\n",
        "print(\"문장의 최장 길이:\", max_len)\n",
        "print(\"문장의 평균 길이:\", sum_len // len(train_data))\n",
        "for p, l in coverage_dict.items():\n",
        "    print(\"문장의 coverage가 \"+ str(p) +\"% 인 길이: \", l)\n",
        "\n",
        "sentence_length = np.zeros((max_len), dtype=int)\n",
        "\n",
        "for sen in train_data['document']:\n",
        "    sentence_length[len(sen)-1] += 1\n",
        "\n",
        "print(len(train_data))\n",
        "\n",
        "plt.bar(range(max_len), sentence_length, width=1.0)\n",
        "plt.title(\"Sentence Length Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "id": "nQdqK4ICxLku",
        "outputId": "b7c2ae8e-dcf2-4f20-f103-9928e475004e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장의 최단 길이: 1\n",
            "문장의 최장 길이: 163\n",
            "문장의 평균 길이: 33\n",
            "문장의 coverage가 50% 인 길이:  43\n",
            "문장의 coverage가 60% 인 길이:  54\n",
            "문장의 coverage가 70% 인 길이:  74\n",
            "문장의 coverage가 80% 인 길이:  97\n",
            "문장의 coverage가 90% 인 길이:  123\n",
            "문장의 coverage가 95% 인 길이:  131\n",
            "문장의 coverage가 99% 인 길이:  139\n",
            "144478\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_sentence_with_length(raw, length, max_print=100):\n",
        "    \"\"\"\n",
        "    지정된 길이의 문장을 최대 max_print개까지 출력합니다.\n",
        "    NaN 또는 비문자열은 자동으로 건너뜁니다.\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "\n",
        "    for sen in raw:\n",
        "        if not isinstance(sen, str):  # NaN 등 비문자열 제외\n",
        "            continue\n",
        "\n",
        "        if len(sen) == length:\n",
        "            print(sen)\n",
        "            count += 1\n",
        "            if count >= max_print:  # 100개 이상이면 종료\n",
        "                break\n",
        "\n",
        "    if count == 0:\n",
        "        print(f\"길이가 {length}인 문장은 없습니다.\")\n",
        "\n",
        "    print(count)\n",
        "\n",
        "check_sentence_with_length(train_data['document'], 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGChfNfQxNed",
        "outputId": "1ee86044-58a2-444e-8347-adb956b0614a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "길이가 0인 문장은 없습니다.\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 중간에 비이상적으로 많은 값 확인 목적\n",
        "\n",
        "# for idx, _sum in enumerate(sentence_length):\n",
        "#     # 문장의 수가 1500을 초과하는 문장 길이를 추출합니다.\n",
        "#     if _sum > 5000:\n",
        "#         print(\"Outlier Index:\", idx+1)"
      ],
      "metadata": {
        "id": "TU7Ys78ixSh2"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_corpus = train_data.drop_duplicates(subset=['document']).reset_index(drop=True)\n",
        "\n",
        "min_len = 999\n",
        "max_len = 0\n",
        "sum_len = 0\n",
        "thr_len = []\n",
        "percent = [50, 60, 70, 80, 90, 95, 99]\n",
        "len_list = []\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "    sen = train_data['document'][i]\n",
        "    length = len(train_data['document'][i])\n",
        "    if min_len > length: min_len = length\n",
        "    if max_len < length: max_len = length\n",
        "    sum_len += length\n",
        "    len_list.append(length)\n",
        "\n",
        "len_list.sort()\n",
        "\n",
        "# XX% coverage -> sentence_lenght 사용해서 다시 진행 필요\n",
        "threshold = []\n",
        "for perc in percent:\n",
        "    threshold.append(sum_len * (perc/100))\n",
        "\n",
        "for thr in threshold:\n",
        "    thr_sum = 0\n",
        "    for length in len_list:\n",
        "        thr_sum += length\n",
        "        if thr_sum > thr:\n",
        "            thr_len.append(length)\n",
        "            break\n",
        "\n",
        "coverage_dict = dict(zip(percent, thr_len))\n",
        "\n",
        "print(\"문장의 최단 길이:\", min_len)\n",
        "print(\"문장의 최장 길이:\", max_len)\n",
        "print(\"문장의 평균 길이:\", sum_len // len(train_data))\n",
        "for p, l in coverage_dict.items():\n",
        "    print(\"문장의 coverage가 \"+ str(p) +\"% 인 길이: \", l)\n",
        "\n",
        "sentence_length = np.zeros((max_len), dtype=int)\n",
        "\n",
        "for sen in train_data['document']:\n",
        "    sentence_length[len(sen)-1] += 1\n",
        "\n",
        "print(len(cleaned_corpus))\n",
        "\n",
        "plt.bar(range(max_len), sentence_length, width=1.0)\n",
        "plt.title(\"Sentence Length Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "id": "f_Ps3ktXxU5u",
        "outputId": "ba1157b5-38be-4c93-b18e-85d4a8b66c08"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장의 최단 길이: 1\n",
            "문장의 최장 길이: 163\n",
            "문장의 평균 길이: 33\n",
            "문장의 coverage가 50% 인 길이:  43\n",
            "문장의 coverage가 60% 인 길이:  54\n",
            "문장의 coverage가 70% 인 길이:  74\n",
            "문장의 coverage가 80% 인 길이:  97\n",
            "문장의 coverage가 90% 인 길이:  123\n",
            "문장의 coverage가 95% 인 길이:  131\n",
            "문장의 coverage가 99% 인 길이:  139\n",
            "144478\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2️⃣ 길이 필터 적용 ---\n",
        "max_len = 131\n",
        "# min_len = 10\n",
        "min_len = 0\n",
        "\n",
        "\n",
        "filtered_corpus = [\n",
        "    s for s in cleaned_corpus['document']\n",
        "    if isinstance(s, str) and (min_len <= len(s) < max_len)\n",
        "]\n",
        "\n",
        "print(f\"필터링된 문장 수: {len(filtered_corpus)} / 전체 {len(cleaned_corpus)}\")\n",
        "\n",
        "# --- 3️⃣ 문장 길이 분포 계산 ---\n",
        "sentence_length = np.zeros((max_len), dtype=int)\n",
        "len_list = []\n",
        "\n",
        "for sen in filtered_corpus:\n",
        "    l = len(sen)\n",
        "    sentence_length[l - 1] += 1\n",
        "    len_list.append(l)\n",
        "\n",
        "# --- 4️⃣ Coverage 계산 ---\n",
        "len_list.sort()\n",
        "sum_len = sum(len_list)\n",
        "percent = [50, 60, 70, 80, 90, 95, 99]\n",
        "thr_len = []\n",
        "\n",
        "for p in percent:\n",
        "    threshold = sum_len * (p / 100)\n",
        "    thr_sum = 0\n",
        "    for l in len_list:\n",
        "        thr_sum += l\n",
        "        if thr_sum > threshold:\n",
        "            thr_len.append(l)\n",
        "            break\n",
        "\n",
        "coverage_dict = dict(zip(percent, thr_len))\n",
        "\n",
        "print(\"문장의 최단 길이:\", min(len_list))\n",
        "print(\"문장의 최장 길이:\", max(len_list))\n",
        "print(\"문장의 평균 길이:\", sum_len // len(len_list))\n",
        "for p, l in coverage_dict.items():\n",
        "    print(f\"문장의 coverage가 {p}%인 길이: {l}\")\n",
        "\n",
        "plt.bar(range(max_len), sentence_length, width=1.0)\n",
        "plt.title(\"Sentence Length Distribution (Filtered)\")\n",
        "plt.xlabel(\"Sentence Length\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "fN9tl-eqxWG2",
        "outputId": "ba36ead7-18a4-4d12-cacf-191ac0fd0b81"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "필터링된 문장 수: 142673 / 전체 144478\n",
            "문장의 최단 길이: 1\n",
            "문장의 최장 길이: 130\n",
            "문장의 평균 길이: 32\n",
            "문장의 coverage가 50%인 길이: 41\n",
            "문장의 coverage가 60%인 길이: 50\n",
            "문장의 coverage가 70%인 길이: 66\n",
            "문장의 coverage가 80%인 길이: 87\n",
            "문장의 coverage가 90%인 길이: 113\n",
            "문장의 coverage가 95%인 길이: 124\n",
            "문장의 coverage가 99%인 길이: 129\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 공백 기반 토큰화"
      ],
      "metadata": {
        "id": "NlILFSx4ySFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, filters=''):\n",
        "        self.word_index = {}\n",
        "        self.index_word = {}\n",
        "        self.filters = filters\n",
        "\n",
        "    def fit_on_texts(self, corpus):\n",
        "        # 각 문장을 토큰화하여 단어 사전을 생성 (인덱스는 1부터 시작)\n",
        "        for sentence in corpus:\n",
        "            tokens = sentence.split() if isinstance(sentence, str) else sentence\n",
        "            for token in tokens:\n",
        "                if token not in self.word_index:\n",
        "                    self.word_index[token] = len(self.word_index) + 1\n",
        "        # 역방향 사전 생성\n",
        "        self.index_word = {idx: word for word, idx in self.word_index.items()}\n",
        "\n",
        "    def texts_to_sequences(self, corpus):\n",
        "        sequences = []\n",
        "        for sentence in corpus:\n",
        "            tokens = sentence.split() if isinstance(sentence, str) else sentence\n",
        "            # 존재하지 않는 단어는 0으로 처리\n",
        "            seq = [self.word_index.get(token, 0) for token in tokens]\n",
        "            sequences.append(torch.tensor(seq, dtype=torch.long))\n",
        "        return sequences\n",
        "\n",
        "    def sequences_to_texts(self, sequences):\n",
        "        texts = []\n",
        "        for seq in sequences:\n",
        "            # tensor인 경우 리스트로 변환\n",
        "            if isinstance(seq, torch.Tensor):\n",
        "                seq = seq.tolist()\n",
        "            # 패딩 토큰(0)은 제외하고 디코딩\n",
        "            tokens = [self.index_word.get(idx, \"\") for idx in seq if idx != 0]\n",
        "            texts.append(tokens)\n",
        "        return texts"
      ],
      "metadata": {
        "id": "QsBAasLTxXh-"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(corpus):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    sequences = tokenizer.texts_to_sequences(corpus)\n",
        "    # padding 토큰은 0\n",
        "    tensor = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
        "    return tensor, tokenizer"
      ],
      "metadata": {
        "id": "uPlOp5nJytH2"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 형태소 기반 토큰화"
      ],
      "metadata": {
        "id": "keWNEMBqzgRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Mecab\n",
        "\n",
        "mecab = Mecab()\n",
        "\n",
        "def mecab_split(sentence):\n",
        "    return mecab.morphs(sentence)\n",
        "\n",
        "mecab_corpus = []\n",
        "\n",
        "for kor in filtered_corpus:\n",
        "    mecab_corpus.append(mecab_split(kor))"
      ],
      "metadata": {
        "id": "2GVcOo0_ytrG"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mecab_tensor, mecab_tokenizer = tokenize(mecab_corpus)\n",
        "\n",
        "print(\"MeCab Vocab Size:\", len(mecab_tokenizer.index_word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKJEhtvJytte",
        "outputId": "fd2afadd-40af-4146-b052-1d8de8ce851a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MeCab Vocab Size: 47326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 라이브러리 버전 확인"
      ],
      "metadata": {
        "id": "qcj9SUB21FAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import konlpy\n",
        "\n",
        "print(torch.__version__)\n",
        "print(np.__version__)\n",
        "print(matplotlib.__version__)\n",
        "print(konlpy.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6mkpq_B1HDW",
        "outputId": "974b3b50-8c9a-4b40-8b10-b1c35763bc9c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0+cu126\n",
            "2.0.2\n",
            "3.10.0\n",
            "0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SentencePiece 모델 학습"
      ],
      "metadata": {
        "id": "TovZ3Q1r12yv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer 함수 작성\n",
        "\n"
      ],
      "metadata": {
        "id": "rgyAwDel17WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sp_tokenize(s, corpus):\n",
        "\n",
        "    tensor = []\n",
        "\n",
        "    for sen in corpus:\n",
        "        tensor.append(s.EncodeAsIds(sen))\n",
        "\n",
        "    with open(\"./korean_spm.vocab\", 'r') as f:\n",
        "        vocab = f.readlines()\n",
        "\n",
        "    word_index = {}\n",
        "    index_word = {}\n",
        "\n",
        "    for idx, line in enumerate(vocab):\n",
        "        word = line.split(\"\\t\")[0]\n",
        "\n",
        "        word_index.update({word:idx})\n",
        "        index_word.update({idx:word})\n",
        "\n",
        "    tensor = pad_sequence(tensor, batch_first=True, padding_value=0)\n",
        "\n",
        "    return tensor, word_index, index_word"
      ],
      "metadata": {
        "id": "JYQokjin2AKo"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Komoran\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "def komoran_tokenize(corpus):\n",
        "    kom = Komoran()\n",
        "    tokenized = []\n",
        "\n",
        "    # 문장별 형태소 단위 토큰화\n",
        "    for sen in corpus:\n",
        "        if not isinstance(sen, str):\n",
        "            continue\n",
        "        tokens = [word for word, pos in kom.pos(sen, norm=True, stem=True)]\n",
        "        tokenized.append(tokens)\n",
        "\n",
        "    # vocab 만들기\n",
        "    counter = {}\n",
        "    for toks in tokenized:\n",
        "        for t in toks:\n",
        "            counter[t] = counter.get(t, 0) + 1\n",
        "\n",
        "    vocab = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
        "    word_index = {w: i + 2 for i, (w, _) in enumerate(vocab)}  # +2 for pad/unk\n",
        "    word_index[\"<pad>\"] = 0\n",
        "    word_index[\"<unk>\"] = 1\n",
        "    index_word = {i: w for w, i in word_index.items()}\n",
        "\n",
        "    # 인덱스 변환\n",
        "    tensor = []\n",
        "    for toks in tokenized:\n",
        "        tensor.append(torch.tensor([word_index.get(t, 1) for t in toks], dtype=torch.long))\n",
        "\n",
        "    # 패딩\n",
        "    tensor = pad_sequence(tensor, batch_first=True, padding_value=0)\n",
        "\n",
        "    return tensor, word_index, index_word\n"
      ],
      "metadata": {
        "id": "_Q0Hrufqjrkg"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from soynlp.word import WordExtractor\n",
        "from soynlp.tokenizer import LTokenizer\n",
        "\n",
        "def soynlp_tokenize(corpus):\n",
        "    # WordExtractor 학습\n",
        "    extractor = WordExtractor(min_frequency=5)\n",
        "    extractor.train(corpus)\n",
        "    word_scores = extractor.extract()\n",
        "    cohesion = {w: s.cohesion_forward for w, s in word_scores.items()}\n",
        "    tokenizer = LTokenizer(cohesion)\n",
        "\n",
        "    tokenized = [tokenizer.tokenize(sen) for sen in corpus if isinstance(sen, str)]\n",
        "\n",
        "    # vocab 만들기\n",
        "    counter = {}\n",
        "    for toks in tokenized:\n",
        "        for t in toks:\n",
        "            counter[t] = counter.get(t, 0) + 1\n",
        "\n",
        "    vocab = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
        "    word_index = {w: i + 2 for i, (w, _) in enumerate(vocab)}  # +2 for pad/unk\n",
        "    word_index[\"<pad>\"] = 0\n",
        "    word_index[\"<unk>\"] = 1\n",
        "    index_word = {i: w for w, i in word_index.items()}\n",
        "\n",
        "    # 인덱스 변환\n",
        "    tensor = []\n",
        "    for toks in tokenized:\n",
        "        tensor.append(torch.tensor([word_index.get(t, 1) for t in toks], dtype=torch.long))\n",
        "\n",
        "    tensor = pad_sequence(tensor, batch_first=True, padding_value=0)\n",
        "\n",
        "    return tensor, word_index, index_word\n"
      ],
      "metadata": {
        "id": "rg3acj-ej6SX"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Common helpers =========================================================\n",
        "from collections import Counter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "from typing import List, Tuple, Dict, Iterable, Callable, Optional\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "def _build_vocab_from_token_lists(token_lists: Iterable[List[str]]):\n",
        "    counter = Counter()\n",
        "    for toks in token_lists:\n",
        "        counter.update(toks)\n",
        "    vocab = [w for w, _ in counter.most_common()]\n",
        "    word_index = {w: i + 2 for i, w in enumerate(vocab)}  # +2 for <pad>, <unk>\n",
        "    word_index[\"<pad>\"] = 0\n",
        "    word_index[\"<unk>\"] = 1\n",
        "    index_word = {i: w for w, i in word_index.items()}\n",
        "    return word_index, index_word\n",
        "\n",
        "def _to_padded_tensor(token_lists: Iterable[List[str]], word_index: Dict[str, int]):\n",
        "    seqs = [torch.tensor([word_index.get(t, 1) for t in toks], dtype=torch.long)\n",
        "            for toks in token_lists]\n",
        "    return pad_sequence(seqs, batch_first=True, padding_value=0)\n",
        "\n",
        "# Small wrapper to make corpus-level API like your komoran/soynlp ones\n",
        "def _tokenize_corpus(\n",
        "    corpus: Iterable[str],\n",
        "    tokenize_one_fn: Callable[[str], List[str]]\n",
        ") -> Tuple[torch.Tensor, Dict[str,int], Dict[int,str]]:\n",
        "    corpus = [s for s in corpus if isinstance(s, str) and s.strip()]\n",
        "    token_lists = [tokenize_one_fn(s) for s in corpus]\n",
        "    word_index, index_word = _build_vocab_from_token_lists(token_lists)\n",
        "    tensor = _to_padded_tensor(token_lists, word_index)\n",
        "    return tensor, word_index, index_word\n"
      ],
      "metadata": {
        "id": "YcVAMNpyotqd"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== OKT ====================================================================\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "_okt_singleton = None\n",
        "def _get_okt():\n",
        "    global _okt_singleton\n",
        "    if _okt_singleton is None:\n",
        "        _okt_singleton = Okt()\n",
        "    return _okt_singleton\n",
        "\n",
        "def okt_tokenize_one(s: str) -> List[str]:\n",
        "    okt = _get_okt()\n",
        "    return [w for w, p in okt.pos(s, norm=True, stem=True)]\n",
        "\n",
        "def okt_pos_pairs_one(s: str):\n",
        "    okt = _get_okt()\n",
        "    return okt.pos(s, norm=True, stem=True)\n",
        "\n",
        "def okt_tokenize(corpus):\n",
        "    return _tokenize_corpus(corpus, okt_tokenize_one)\n"
      ],
      "metadata": {
        "id": "O57ZIduLotuO"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Hannanum ===============================================================\n",
        "from konlpy.tag import Hannanum\n",
        "\n",
        "_hannanum_singleton = None\n",
        "def _get_hannanum():\n",
        "    global _hannanum_singleton\n",
        "    if _hannanum_singleton is None:\n",
        "        _hannanum_singleton = Hannanum()\n",
        "    return _hannanum_singleton\n",
        "\n",
        "def hannanum_tokenize_one(s: str) -> List[str]:\n",
        "    han = _get_hannanum()\n",
        "    # Hannanum.pos returns list[(token, tag)]\n",
        "    return [w for w, p in han.pos(s)]\n",
        "\n",
        "def hannanum_pos_pairs_one(s: str):\n",
        "    han = _get_hannanum()\n",
        "    return han.pos(s)\n",
        "\n",
        "def hannanum_tokenize(corpus):\n",
        "    return _tokenize_corpus(corpus, hannanum_tokenize_one)\n"
      ],
      "metadata": {
        "id": "OmlA1RILotxW"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Kkma ===================================================================\n",
        "from konlpy.tag import Kkma\n",
        "\n",
        "_kkma_singleton = None\n",
        "def _get_kkma():\n",
        "    global _kkma_singleton\n",
        "    if _kkma_singleton is None:\n",
        "        _kkma_singleton = Kkma()\n",
        "    return _kkma_singleton\n",
        "\n",
        "def kkma_tokenize_one(s: str) -> List[str]:\n",
        "    kk = _get_kkma()\n",
        "    return [w for w, p in kk.pos(s)]\n",
        "\n",
        "def kkma_pos_pairs_one(s: str):\n",
        "    kk = _get_kkma()\n",
        "    return kk.pos(s)\n",
        "\n",
        "def kkma_tokenize(corpus):\n",
        "    return _tokenize_corpus(corpus, kkma_tokenize_one)\n"
      ],
      "metadata": {
        "id": "Q1YQ5cjQot0l"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Mecab (konlpy) =========================================================\n",
        "# Make sure mecab-ko is installed. If not available via konlpy, fall back gracefully.\n",
        "try:\n",
        "    from konlpy.tag import Mecab\n",
        "    _mecab_singleton = None\n",
        "    def _get_mecab():\n",
        "        global _mecab_singleton\n",
        "        if _mecab_singleton is None:\n",
        "            _mecab_singleton = Mecab()\n",
        "        return _mecab_singleton\n",
        "\n",
        "    def mecab_tokenize_one(s: str) -> List[str]:\n",
        "        mc = _get_mecab()\n",
        "        return [w for w, p in mc.pos(s)]\n",
        "\n",
        "    def mecab_pos_pairs_one(s: str):\n",
        "        mc = _get_mecab()\n",
        "        return mc.pos(s)\n",
        "\n",
        "    def mecab_tokenize(corpus):\n",
        "        return _tokenize_corpus(corpus, mecab_tokenize_one)\n",
        "\n",
        "except Exception as e:\n",
        "    # Optional stub if Mecab not available\n",
        "    def mecab_tokenize_one(s: str) -> List[str]:\n",
        "        raise RuntimeError(\"Mecab not available via konlpy. Install mecab-ko & konlpy Mecab.\") from e\n",
        "    def mecab_pos_pairs_one(s: str):\n",
        "        return None\n",
        "    def mecab_tokenize(corpus):\n",
        "        raise RuntimeError(\"Mecab not available via konlpy. Install mecab-ko & konlpy Mecab.\") from e\n"
      ],
      "metadata": {
        "id": "eXJfHJecot4V"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== SentencePiece (Unigram/BPE, separate prefix version) ====================\n",
        "import os, tempfile\n",
        "from typing import Iterable, Optional, List, Callable, Tuple\n",
        "import sentencepiece as spm\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 1) Train\n",
        "# -------------------------------------------------------------------\n",
        "def spm_train_model_from_corpus(\n",
        "    corpus: Iterable[str],\n",
        "    model_prefix: str,\n",
        "    vocab_size: int = 8000,\n",
        "    model_type: str = \"unigram\",  # \"unigram\" or \"bpe\"\n",
        "    character_coverage: float = 0.9995,\n",
        "    user_defined_symbols: Optional[List[str]] = None,\n",
        "    add_bos_eos: bool = False,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Writes a temp txt, trains SPM, returns model path '<prefix>.model'\n",
        "    \"\"\"\n",
        "    corpus = [s for s in corpus if isinstance(s, str) and s.strip()]\n",
        "    if not corpus:\n",
        "        raise ValueError(\"Empty corpus for SentencePiece training.\")\n",
        "\n",
        "    os.makedirs(os.path.dirname(model_prefix) or \".\", exist_ok=True)\n",
        "    tmp_txt = model_prefix + \".txt\"\n",
        "    with open(tmp_txt, \"w\", encoding=\"utf-8\") as f:\n",
        "        for line in corpus:\n",
        "            f.write(line.replace(\"\\n\", \" \").strip() + \"\\n\")\n",
        "\n",
        "    uds = \",\".join(user_defined_symbols) if user_defined_symbols else \"\"\n",
        "    args = [\n",
        "        f\"--input={tmp_txt}\",\n",
        "        f\"--model_prefix={model_prefix}\",\n",
        "        f\"--vocab_size={vocab_size}\",\n",
        "        f\"--model_type={model_type}\",\n",
        "        f\"--character_coverage={character_coverage}\",\n",
        "        \"--pad_id=-1\",  # manage <pad> manually\n",
        "    ]\n",
        "    if uds:\n",
        "        args.append(f\"--user_defined_symbols={uds}\")\n",
        "    if add_bos_eos:\n",
        "        args.append(\"--bos_id=1\")\n",
        "        args.append(\"--eos_id=2\")\n",
        "    else:\n",
        "        args.append(\"--bos_id=-1\")\n",
        "        args.append(\"--eos_id=-1\")\n",
        "\n",
        "    spm.SentencePieceTrainer.Train(\" \".join(args))\n",
        "    return model_prefix + \".model\"\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2) Helpers\n",
        "# -------------------------------------------------------------------\n",
        "def spm_load(model_path: str) -> spm.SentencePieceProcessor:\n",
        "    sp = spm.SentencePieceProcessor()\n",
        "    sp.Load(model_path)\n",
        "    return sp\n",
        "\n",
        "def spm_tokenize_one(sp: spm.SentencePieceProcessor, s: str) -> List[str]:\n",
        "    return sp.EncodeAsPieces(s)\n",
        "\n",
        "def spm_build_word_index_from_model(sp: spm.SentencePieceProcessor):\n",
        "    size = sp.GetPieceSize()\n",
        "    pieces = [sp.IdToPiece(i) for i in range(size)]\n",
        "    core = [p for p in pieces if p != \"<unk>\"]\n",
        "    word_index = {w: i + 2 for i, w in enumerate(core)}\n",
        "    word_index[\"<pad>\"] = 0\n",
        "    word_index[\"<unk>\"] = 1\n",
        "    index_word = {i: w for w, i in word_index.items()}\n",
        "    return word_index, index_word\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3) Main function\n",
        "# -------------------------------------------------------------------\n",
        "def sentencepiece_tokenize(\n",
        "    corpus: Iterable[str],\n",
        "    model_type: str = \"unigram\",  # or \"bpe\"\n",
        "    vocab_size: int = 8000,\n",
        "    character_coverage: float = 0.9995,\n",
        "    user_defined_symbols: Optional[List[str]] = None,\n",
        "    add_bos_eos: bool = False,\n",
        "    output_dir: str = \"./spm_models\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Train and tokenize with SentencePiece.\n",
        "    Each model_type automatically uses a distinct prefix under output_dir.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    model_prefix = os.path.join(output_dir, f\"ko_{model_type}_{vocab_size}\")\n",
        "    model_path = model_prefix + \".model\"\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        model_path = spm_train_model_from_corpus(\n",
        "            corpus=corpus,\n",
        "            model_prefix=model_prefix,\n",
        "            vocab_size=vocab_size,\n",
        "            model_type=model_type,\n",
        "            character_coverage=character_coverage,\n",
        "            user_defined_symbols=user_defined_symbols,\n",
        "            add_bos_eos=add_bos_eos,\n",
        "        )\n",
        "\n",
        "    sp = spm_load(model_path)\n",
        "    corpus = [s for s in corpus if isinstance(s, str) and s.strip()]\n",
        "    token_lists = [sp.EncodeAsPieces(s) for s in corpus]\n",
        "\n",
        "    word_index, index_word = spm_build_word_index_from_model(sp)\n",
        "    seqs = [torch.tensor([word_index.get(t, 1) for t in toks], dtype=torch.long)\n",
        "            for toks in token_lists]\n",
        "    tensor = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
        "    return tensor, word_index, index_word, model_path, sp\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 4) Wrapper shortcuts\n",
        "# -------------------------------------------------------------------\n",
        "def sentencepiece_unigram_tokenize(corpus, **kwargs):\n",
        "    return sentencepiece_tokenize(corpus, model_type=\"unigram\", **kwargs)\n",
        "\n",
        "def sentencepiece_bpe_tokenize(corpus, **kwargs):\n",
        "    return sentencepiece_tokenize(corpus, model_type=\"bpe\", **kwargs)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 5) analyze_tokenizer용 단일 문장 함수\n",
        "# -------------------------------------------------------------------\n",
        "def spm_make_tokenize_one(model_path: str) -> Callable[[str], List[str]]:\n",
        "    sp = spm_load(model_path)\n",
        "    def _fn(s: str) -> List[str]:\n",
        "        return sp.EncodeAsPieces(s)\n",
        "    return _fn\n",
        "\n",
        "def spm_pos_pairs_one(_unused: str):\n",
        "    return None  # SentencePiece는 품사 정보 없음\n"
      ],
      "metadata": {
        "id": "inNTJ5WGot-l"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5G0Bwr1QouB1"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8k-B0KCPouFn"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zbBvEFAtouJN"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "def analyze_tokenizer(\n",
        "    corpus,\n",
        "    tokenize_one_fn,                 # (str) -> list[str]\n",
        "    pos_pairs_one_fn=None,           # (str) -> list[(token, pos)] | None\n",
        "    top_n=20,\n",
        "    coverages=(0.9, 0.95, 0.99),\n",
        "    example_sentences=None,          # ✅ 새로 추가: 예시 문장 리스트\n",
        "    title=\"Tokenizer Report\",\n",
        "):\n",
        "    \"\"\"\n",
        "    - 코퍼스로 통계/coverage/상위 토큰 계산\n",
        "    - 예시 문장은 example_sentences로 별도 받아 토큰화 결과만 출력\n",
        "    \"\"\"\n",
        "    # 0) 코퍼스 정리\n",
        "    corpus = [s for s in corpus if isinstance(s, str) and s.strip()]\n",
        "    if not corpus:\n",
        "        print(f\"[{title}] 빈 코퍼스입니다.\")\n",
        "        return {}\n",
        "\n",
        "    # 1) 코퍼스 토큰화\n",
        "    token_lists = [tokenize_one_fn(s) for s in corpus]\n",
        "    flat_tokens = [t for toks in token_lists for t in toks]\n",
        "\n",
        "    # 2) 기본 통계\n",
        "    total_tokens = len(flat_tokens)\n",
        "    avg_tokens_per_sent = float(np.mean([len(toks) for toks in token_lists])) if token_lists else 0.0\n",
        "    avg_token_char_len = float(np.mean([len(t) for t in flat_tokens])) if flat_tokens else 0.0\n",
        "\n",
        "    # 3) 품사 비율 (옵션)\n",
        "    pos_ratio = None\n",
        "    if pos_pairs_one_fn is not None:\n",
        "        pos_counter = Counter()\n",
        "        for s in corpus:\n",
        "            pairs = pos_pairs_one_fn(s) or []\n",
        "            pos_counter.update([p for _, p in pairs])\n",
        "        total_pos = sum(pos_counter.values())\n",
        "        if total_pos > 0:\n",
        "            pos_ratio = {p: round(c / total_pos, 4) for p, c in pos_counter.items()}\n",
        "\n",
        "    # 4) 상위 토큰\n",
        "    top_tokens = Counter(flat_tokens).most_common(top_n)\n",
        "\n",
        "    # 5) coverage 기반 vocab size\n",
        "    freq = Counter(flat_tokens)\n",
        "    total_freq = sum(freq.values())\n",
        "    sorted_items = freq.most_common()\n",
        "    suggested_vocab_size = {}\n",
        "    for cov in coverages:\n",
        "        need = total_freq * cov\n",
        "        acc = 0\n",
        "        k = 0\n",
        "        for _, c in sorted_items:\n",
        "            acc += c\n",
        "            k += 1\n",
        "            if acc >= need:\n",
        "                suggested_vocab_size[cov] = k\n",
        "                break\n",
        "\n",
        "    # 6) 요약 출력\n",
        "    print(f\"\\n[{title}] 요약\")\n",
        "    print(f\"- 전체 문장 수: {len(corpus)}\")\n",
        "    print(f\"- 전체 토큰 수: {total_tokens}\")\n",
        "    print(f\"- 문장당 평균 토큰 수: {avg_tokens_per_sent:.2f}\")\n",
        "    print(f\"- 평균 토큰 '문자' 길이: {avg_token_char_len:.2f}\")\n",
        "    if pos_ratio is not None:\n",
        "        print(\"- 품사 비율:\")\n",
        "        for p, r in sorted(pos_ratio.items(), key=lambda x: (-x[1], x[0])):\n",
        "            print(f\"  • {p}: {r:.4f}\")\n",
        "\n",
        "    print(f\"\\n- 상위 {top_n} 토큰:\")\n",
        "    for tok, cnt in top_tokens:\n",
        "        print(f\"  • \\\"{tok}\\\" {cnt}개\")\n",
        "\n",
        "    print(\"\\n- coverage 기준 vocab size 제안:\")\n",
        "    for cov in coverages:\n",
        "        k = suggested_vocab_size.get(cov, 0)\n",
        "        print(f\"  • {int(cov*100)}% → {k}\")\n",
        "\n",
        "    # 7) 예시 문장 토큰화 (코퍼스와 별개로 처리) ✅\n",
        "    if example_sentences:\n",
        "        print(f\"\\n- 예시 문장 {min(len(example_sentences), 5)}개 토큰화:\")\n",
        "        for i, s in enumerate(example_sentences[:5], 1):\n",
        "            print(f\"  {i}. {s}\")\n",
        "            toks = tokenize_one_fn(s)\n",
        "            if pos_pairs_one_fn is not None:\n",
        "                pairs = pos_pairs_one_fn(s) or []\n",
        "                print(\"     -> tokens:\", toks)\n",
        "                print(\"     -> pos   :\", pairs)\n",
        "            else:\n",
        "                print(\"     ->\", toks)\n",
        "\n",
        "    # 8) 머신 친화 반환\n",
        "    return {\n",
        "        \"total_tokens\": total_tokens,\n",
        "        \"avg_tokens_per_sentence\": avg_tokens_per_sent,\n",
        "        \"avg_token_char_length\": avg_token_char_len,\n",
        "        \"pos_ratio\": pos_ratio,\n",
        "        \"top_tokens\": top_tokens,\n",
        "        \"suggested_vocab_size\": suggested_vocab_size,\n",
        "        \"examples\": [\n",
        "            (s, tokenize_one_fn(s),\n",
        "             (pos_pairs_one_fn(s) if pos_pairs_one_fn else None))\n",
        "            for s in (example_sentences[:5] if example_sentences else [])\n",
        "        ],\n",
        "    }\n"
      ],
      "metadata": {
        "id": "UBv00GW0lLH4"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_sents = [\"오늘 날씨가 정말 좋네요.\",\n",
        "             \"저는 자연어 처리를 공부하고 있습니다.\",\n",
        "             \"ELMo와 BERT는 문맥 기반 임베딩 모델입니다.\",\n",
        "             \"파이썬으로 데이터를 전처리하고 시각화합니다.\",\n",
        "             \"모델 성능을 높이기 위해 하이퍼파라미터를 조정합니다.\"]"
      ],
      "metadata": {
        "id": "9l-z4TFav8ao"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# s = spm.SentencePieceProcessor(); s.Load('korean_spm.model')\n",
        "\n",
        "# spm_tokenize_one = lambda text: s.EncodeAsPieces(text)\n",
        "\n",
        "# _ = analyze_tokenizer(\n",
        "#     corpus=filtered_corpus,\n",
        "#     tokenize_one_fn=spm_tokenize_one,\n",
        "#     pos_pairs_one_fn=None,              # SP는 품사 없음\n",
        "#     example_sentences=example_sents,\n",
        "#     title=\"SentencePiece\"\n",
        "# )\n"
      ],
      "metadata": {
        "id": "w7bJqJQ_lNxR"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Komoran\n",
        "kom = Komoran()\n",
        "\n",
        "# (필터링된 토큰)과 (품사쌍) 둘 다 준비\n",
        "def komoran_tokens(text, include_pos=(\"NNG\",\"NNP\",\"VV\",\"VA\",\"MAG\",\"MAJ\")):\n",
        "    pairs = kom.pos(text)\n",
        "    return [w for (w, p) in pairs if p in include_pos]\n",
        "\n",
        "def komoran_pos_pairs(text):\n",
        "    return kom.pos(text)  # 품사 비율 계산용\n",
        "\n",
        "_ = analyze_tokenizer(\n",
        "    corpus=filtered_corpus,\n",
        "    tokenize_one_fn=komoran_tokens,\n",
        "    pos_pairs_one_fn=komoran_pos_pairs,\n",
        "    example_sentences=example_sents,\n",
        "    title=\"Komoran\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbjRy9LhlN0P",
        "outputId": "7805d12a-338e-446c-9acd-ed97530b7057"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Komoran] 요약\n",
            "- 전체 문장 수: 142673\n",
            "- 전체 토큰 수: 1135037\n",
            "- 문장당 평균 토큰 수: 7.96\n",
            "- 평균 토큰 '문자' 길이: 1.87\n",
            "- 품사 비율:\n",
            "  • NNG: 0.1698\n",
            "  • EC: 0.1338\n",
            "  • NNP: 0.1154\n",
            "  • VV: 0.0911\n",
            "  • SF: 0.0640\n",
            "  • MAG: 0.0494\n",
            "  • ETM: 0.0464\n",
            "  • VA: 0.0402\n",
            "  • NNB: 0.0362\n",
            "  • EP: 0.0274\n",
            "  • JX: 0.0209\n",
            "  • VX: 0.0192\n",
            "  • VCP: 0.0188\n",
            "  • JKB: 0.0164\n",
            "  • XSV: 0.0154\n",
            "  • ETN: 0.0141\n",
            "  • XSN: 0.0139\n",
            "  • NP: 0.0123\n",
            "  • MM: 0.0119\n",
            "  • XR: 0.0116\n",
            "  • JKO: 0.0109\n",
            "  • SN: 0.0100\n",
            "  • XSA: 0.0092\n",
            "  • SP: 0.0080\n",
            "  • NA: 0.0072\n",
            "  • SL: 0.0050\n",
            "  • JKS: 0.0046\n",
            "  • MAJ: 0.0039\n",
            "  • VCN: 0.0027\n",
            "  • JC: 0.0023\n",
            "  • NR: 0.0023\n",
            "  • XPN: 0.0022\n",
            "  • IC: 0.0018\n",
            "  • JKG: 0.0012\n",
            "  • JKV: 0.0003\n",
            "  • EF: 0.0002\n",
            "  • JKC: 0.0001\n",
            "  • SW: 0.0000\n",
            "\n",
            "- 상위 20 토큰:\n",
            "  • \"영화\" 51464개\n",
            "  • \"보\" 39216개\n",
            "  • \"하\" 18155개\n",
            "  • \"없\" 16801개\n",
            "  • \"있\" 12292개\n",
            "  • \"좋\" 11413개\n",
            "  • \"너무\" 10421개\n",
            "  • \"정말\" 9316개\n",
            "  • \"진짜\" 7957개\n",
            "  • \"재밌\" 7944개\n",
            "  • \"안\" 7706개\n",
            "  • \"연기\" 6882개\n",
            "  • \"같\" 6704개\n",
            "  • \"만들\" 6665개\n",
            "  • \"평점\" 5969개\n",
            "  • \"최고\" 5909개\n",
            "  • \"되\" 5753개\n",
            "  • \"왜\" 5659개\n",
            "  • \"나오\" 5567개\n",
            "  • \"다\" 5146개\n",
            "\n",
            "- coverage 기준 vocab size 제안:\n",
            "  • 90% → 3747\n",
            "  • 95% → 7401\n",
            "  • 99% → 18454\n",
            "\n",
            "- 예시 문장 5개 토큰화:\n",
            "  1. 오늘 날씨가 정말 좋네요.\n",
            "     -> tokens: ['오늘', '날씨', '정말', '좋']\n",
            "     -> pos   : [('오늘', 'NNG'), ('날씨', 'NNG'), ('가', 'JKS'), ('정말', 'MAG'), ('좋', 'VA'), ('네요', 'EF'), ('.', 'SF')]\n",
            "  2. 저는 자연어 처리를 공부하고 있습니다.\n",
            "     -> tokens: ['자연어', '처리', '공부']\n",
            "     -> pos   : [('저', 'NP'), ('는', 'JX'), ('자연어', 'NNP'), ('처리', 'NNG'), ('를', 'JKO'), ('공부', 'NNG'), ('하', 'XSV'), ('고', 'EC'), ('있', 'VX'), ('습니다', 'EF'), ('.', 'SF')]\n",
            "  3. ELMo와 BERT는 문맥 기반 임베딩 모델입니다.\n",
            "     -> tokens: ['문맥', '기반', '임', '베', '딩', '모델']\n",
            "     -> pos   : [('ELMo', 'SL'), ('와', 'JC'), ('BERT', 'SL'), ('는', 'JX'), ('문맥', 'NNG'), ('기반', 'NNG'), ('임', 'NNP'), ('베', 'NNG'), ('딩', 'MAG'), ('모델', 'NNG'), ('이', 'VCP'), ('ㅂ니다', 'EF'), ('.', 'SF')]\n",
            "  4. 파이썬으로 데이터를 전처리하고 시각화합니다.\n",
            "     -> tokens: ['파이썬', '데이터', '전', '처리', '시각']\n",
            "     -> pos   : [('파이썬', 'NNP'), ('으로', 'JKB'), ('데이터', 'NNG'), ('를', 'JKO'), ('전', 'NNG'), ('처리', 'NNG'), ('하', 'XSV'), ('고', 'EC'), ('시각', 'NNG'), ('화', 'XSN'), ('하', 'XSV'), ('ㅂ니다', 'EF'), ('.', 'SF')]\n",
            "  5. 모델 성능을 높이기 위해 하이퍼파라미터를 조정합니다.\n",
            "     -> tokens: ['모델', '성능', '높이', '위하', '하이퍼', '파라', '미터', '조정']\n",
            "     -> pos   : [('모델', 'NNP'), ('성능', 'NNG'), ('을', 'JKO'), ('높이', 'VV'), ('기', 'ETN'), ('위하', 'VV'), ('아', 'EC'), ('하이퍼', 'NNP'), ('파라', 'NNP'), ('미터', 'NNP'), ('를', 'JKO'), ('조정', 'NNG'), ('하', 'XSV'), ('ㅂ니다', 'EF'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from soynlp.word import WordExtractor\n",
        "from soynlp.tokenizer import LTokenizer\n",
        "\n",
        "# soynlp 토크나이저 학습 (코퍼스 크기에 비례해 시간 걸릴 수 있음)\n",
        "extractor = WordExtractor(min_frequency=5)\n",
        "extractor.train(filtered_corpus)\n",
        "word_scores = extractor.extract()\n",
        "cohesion = {w: s.cohesion_forward for w, s in word_scores.items()}\n",
        "soy_tokenizer = LTokenizer(cohesion)\n",
        "\n",
        "soy_tokenize_one = lambda text: soy_tokenizer.tokenize(text)\n",
        "\n",
        "_ = analyze_tokenizer(\n",
        "    corpus=filtered_corpus,\n",
        "    tokenize_one_fn=soy_tokenize_one,\n",
        "    pos_pairs_one_fn=None,\n",
        "    example_sentences=example_sents,\n",
        "    title=\"soynlp\"\n",
        ")\n",
        "\n",
        "# 기본적으로 품사 없음\n",
        "# 품사 추출기 사용을 위해서는 사전에 정의된 dict 필요"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6YvywA5lN3S",
        "outputId": "d3a8877d-0894-4970-b8b2-81cc520798fd"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training was done. used memory 2.866 Gb\n",
            "all cohesion probabilities was computed. # words = 61951\n",
            "all branching entropies was computed # words = 77181\n",
            "all accessor variety was computed # words = 77181\n",
            "\n",
            "[soynlp] 요약\n",
            "- 전체 문장 수: 142673\n",
            "- 전체 토큰 수: 1540858\n",
            "- 문장당 평균 토큰 수: 10.80\n",
            "- 평균 토큰 '문자' 길이: 2.29\n",
            "\n",
            "- 상위 20 토큰:\n",
            "  • \".\" 124359개\n",
            "  • \"영화\" 43691개\n",
            "  • \",\" 19315개\n",
            "  • \"!\" 16563개\n",
            "  • \"?\" 13730개\n",
            "  • \"너무\" 11177개\n",
            "  • \"재미\" 10058개\n",
            "  • \"다\" 9707개\n",
            "  • \"정말\" 9240개\n",
            "  • \"을\" 8429개\n",
            "  • \"재밌\" 7551개\n",
            "  • \"진짜\" 7533개\n",
            "  • \"연기\" 6994개\n",
            "  • \"고\" 6700개\n",
            "  • \"최고\" 5856개\n",
            "  • \"평점\" 5676개\n",
            "  • \"생각\" 4958개\n",
            "  • \"이런\" 4885개\n",
            "  • \"스토리\" 4865개\n",
            "  • \"게\" 4539개\n",
            "\n",
            "- coverage 기준 vocab size 제안:\n",
            "  • 90% → 26877\n",
            "  • 95% → 81567\n",
            "  • 99% → 143201\n",
            "\n",
            "- 예시 문장 5개 토큰화:\n",
            "  1. 오늘 날씨가 정말 좋네요.\n",
            "     -> ['오늘', '날씨가', '정말', '좋네요', '.']\n",
            "  2. 저는 자연어 처리를 공부하고 있습니다.\n",
            "     -> ['저는', '자연', '어', '처리를', '공부하고', '있습니다', '.']\n",
            "  3. ELMo와 BERT는 문맥 기반 임베딩 모델입니다.\n",
            "     -> ['ELMo와', 'BERT는', '문맥', '기반', '임베딩', '모델입니다.']\n",
            "  4. 파이썬으로 데이터를 전처리하고 시각화합니다.\n",
            "     -> ['파이', '썬으로', '데이', '터를', '전처리하고', '시각화합니다.']\n",
            "  5. 모델 성능을 높이기 위해 하이퍼파라미터를 조정합니다.\n",
            "     -> ['모델', '성능을', '높이기', '위해', '하이퍼파라미터를', '조정합니다.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stats_mecab = analyze_tokenizer(\n",
        "    corpus=filtered_corpus,\n",
        "    tokenize_one_fn=mecab_tokenize_one,\n",
        "    pos_pairs_one_fn=mecab_pos_pairs_one,\n",
        "    example_sentences=example_sents,\n",
        "    title=\"Mecab Report\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nMCO2Mxo4Ww",
        "outputId": "4890cb69-e532-481f-85ce-41fa4df148e1"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Mecab Report] 요약\n",
            "- 전체 문장 수: 142673\n",
            "- 전체 토큰 수: 2228335\n",
            "- 문장당 평균 토큰 수: 15.62\n",
            "- 평균 토큰 '문자' 길이: 1.59\n",
            "- 품사 비율:\n",
            "  • NNG: 0.2494\n",
            "  • EC: 0.0796\n",
            "  • MAG: 0.0671\n",
            "  • SF: 0.0595\n",
            "  • VV: 0.0525\n",
            "  • VA: 0.0392\n",
            "  • EF: 0.0311\n",
            "  • NNP: 0.0293\n",
            "  • NNB: 0.0221\n",
            "  • ETM: 0.0211\n",
            "  • JKB: 0.0187\n",
            "  • XSN: 0.0174\n",
            "  • JX: 0.0156\n",
            "  • VX: 0.0155\n",
            "  • MM: 0.0142\n",
            "  • VV+EC: 0.0133\n",
            "  • EP: 0.0128\n",
            "  • VV+ETM: 0.0128\n",
            "  • SN: 0.0122\n",
            "  • NP: 0.0110\n",
            "  • XR: 0.0108\n",
            "  • JKO: 0.0107\n",
            "  • NNBC: 0.0107\n",
            "  • SY: 0.0105\n",
            "  • VCP: 0.0087\n",
            "  • XSV: 0.0086\n",
            "  • SC: 0.0081\n",
            "  • JKS: 0.0077\n",
            "  • IC: 0.0074\n",
            "  • VV+EP: 0.0073\n",
            "  • ETN: 0.0071\n",
            "  • VCP+EC: 0.0061\n",
            "  • XSA: 0.0057\n",
            "  • VA+ETM: 0.0056\n",
            "  • SL: 0.0055\n",
            "  • VCP+EF: 0.0051\n",
            "  • MAJ: 0.0039\n",
            "  • VCP+ETM: 0.0039\n",
            "  • VX+EP: 0.0036\n",
            "  • NR: 0.0034\n",
            "  • VV+EF: 0.0032\n",
            "  • VX+ETM: 0.0032\n",
            "  • XSV+EC: 0.0030\n",
            "  • XPN: 0.0026\n",
            "  • XSA+ETM: 0.0026\n",
            "  • XSV+ETM: 0.0025\n",
            "  • JC: 0.0022\n",
            "  • NP+JX: 0.0022\n",
            "  • JKG: 0.0020\n",
            "  • VCN: 0.0020\n",
            "  • NNB+JKS: 0.0019\n",
            "  • UNKNOWN: 0.0017\n",
            "  • VX+EC: 0.0016\n",
            "  • XSA+ETN: 0.0016\n",
            "  • NNB+JX: 0.0015\n",
            "  • XSV+EP: 0.0015\n",
            "  • JKB+JX: 0.0014\n",
            "  • VA+EC: 0.0014\n",
            "  • NP+JKS: 0.0013\n",
            "  • VCP+EP: 0.0013\n",
            "  • VV+ETN: 0.0013\n",
            "  • VCP+ETN: 0.0012\n",
            "  • XSA+EC: 0.0012\n",
            "  • VX+EF: 0.0011\n",
            "  • XSA+EP: 0.0011\n",
            "  • NP+JKG: 0.0010\n",
            "  • XSV+EF: 0.0010\n",
            "  • VA+EP: 0.0009\n",
            "  • EP+EF: 0.0008\n",
            "  • NNB+JKO: 0.0008\n",
            "  • NP+JKO: 0.0008\n",
            "  • NP+VCP+EC: 0.0007\n",
            "  • ETN+JX: 0.0006\n",
            "  • NNB+VCP+EC: 0.0006\n",
            "  • VA+ETN: 0.0006\n",
            "  • VCN+ETM: 0.0006\n",
            "  • ETN+JKO: 0.0005\n",
            "  • NNB+VCP+EF: 0.0005\n",
            "  • XSA+EF: 0.0005\n",
            "  • XSV+ETN: 0.0005\n",
            "  • EC+JX: 0.0004\n",
            "  • VV+EC+VX+ETM: 0.0004\n",
            "  • EP+ETM: 0.0003\n",
            "  • JKV: 0.0003\n",
            "  • NNG+JX: 0.0003\n",
            "  • VA+EF: 0.0003\n",
            "  • VX+ETN: 0.0003\n",
            "  • EP+EP: 0.0002\n",
            "  • JKC: 0.0002\n",
            "  • NNB+JKC: 0.0002\n",
            "  • NNB+VCP: 0.0002\n",
            "  • VCN+EC: 0.0002\n",
            "  • VCN+EF: 0.0002\n",
            "  • VV+EC+VX+EC: 0.0002\n",
            "  • VV+EC+VX+EP: 0.0002\n",
            "  • VX+EP+EF: 0.0002\n",
            "  • EC+VV: 0.0001\n",
            "  • EC+VX+EC: 0.0001\n",
            "  • EC+VX+EP: 0.0001\n",
            "  • EC+VX+ETM: 0.0001\n",
            "  • EP+EC: 0.0001\n",
            "  • JKQ: 0.0001\n",
            "  • JX+JX: 0.0001\n",
            "  • MAG+JX: 0.0001\n",
            "  • NNB+JKB: 0.0001\n",
            "  • NNG+VCP+EC: 0.0001\n",
            "  • NP+JKB: 0.0001\n",
            "  • NP+VCP+EF: 0.0001\n",
            "  • VCN+ETN: 0.0001\n",
            "  • VV+EC+VX: 0.0001\n",
            "  • VV+EC+VX+EF: 0.0001\n",
            "  • VV+EC+VX+ETN: 0.0001\n",
            "  • VV+EP+EC: 0.0001\n",
            "  • VX+EP+EC: 0.0001\n",
            "  • XSA+EC+VX+ETM: 0.0001\n",
            "  • XSA+EP+EC: 0.0001\n",
            "  • XSN+VCP+ETM: 0.0001\n",
            "  • XSV+EP+EC: 0.0001\n",
            "  • EC+EF: 0.0000\n",
            "  • EC+EP: 0.0000\n",
            "  • EC+JKO: 0.0000\n",
            "  • EC+JKS: 0.0000\n",
            "  • EC+JX+VX+EP: 0.0000\n",
            "  • EC+VA+ETM: 0.0000\n",
            "  • EC+VCP: 0.0000\n",
            "  • EC+VCP+EC: 0.0000\n",
            "  • EC+VCP+EF: 0.0000\n",
            "  • EC+VCP+ETM: 0.0000\n",
            "  • EC+VV+EC: 0.0000\n",
            "  • EC+VV+EC+VX: 0.0000\n",
            "  • EC+VV+EF: 0.0000\n",
            "  • EC+VV+EP: 0.0000\n",
            "  • EC+VV+EP+EC: 0.0000\n",
            "  • EC+VV+EP+EF: 0.0000\n",
            "  • EC+VV+ETM: 0.0000\n",
            "  • EC+VV+ETN: 0.0000\n",
            "  • EC+VV+JX: 0.0000\n",
            "  • EC+VX: 0.0000\n",
            "  • EC+VX+EC+JKO: 0.0000\n",
            "  • EC+VX+EC+VX: 0.0000\n",
            "  • EC+VX+EC+VX+ETM: 0.0000\n",
            "  • EC+VX+EF: 0.0000\n",
            "  • EC+VX+EP+EC: 0.0000\n",
            "  • EC+VX+EP+EF: 0.0000\n",
            "  • EC+VX+EP+ETM: 0.0000\n",
            "  • EC+VX+ETN: 0.0000\n",
            "  • EC+VX+ETN+JKO: 0.0000\n",
            "  • EC+VX+ETN+VCP+EC: 0.0000\n",
            "  • EC+XSV+ETM: 0.0000\n",
            "  • EP+EC+VX: 0.0000\n",
            "  • EP+EF+VCP: 0.0000\n",
            "  • EP+EP+EC: 0.0000\n",
            "  • EP+EP+EC+VX+EF: 0.0000\n",
            "  • EP+EP+EF: 0.0000\n",
            "  • EP+EP+ETM: 0.0000\n",
            "  • EP+ETN: 0.0000\n",
            "  • EP+ETN+JKB: 0.0000\n",
            "  • EP+ETN+JKO: 0.0000\n",
            "  • EP+NA: 0.0000\n",
            "  • ETM+NNB: 0.0000\n",
            "  • ETM+NNB+JKB: 0.0000\n",
            "  • ETM+NNB+JKG: 0.0000\n",
            "  • ETM+NNB+JKS: 0.0000\n",
            "  • ETM+NNB+JX: 0.0000\n",
            "  • ETM+NNB+VCP: 0.0000\n",
            "  • ETM+NNB+VCP+EC: 0.0000\n",
            "  • ETM+NNB+VCP+EF: 0.0000\n",
            "  • ETM+NNB+XSA+EC: 0.0000\n",
            "  • ETM+NNB+XSA+EF: 0.0000\n",
            "  • ETM+NNB+XSA+ETM: 0.0000\n",
            "  • ETM+NNG+JKS: 0.0000\n",
            "  • ETM+NNG+VCP: 0.0000\n",
            "  • ETN+EC: 0.0000\n",
            "  • ETN+EF: 0.0000\n",
            "  • ETN+ETM: 0.0000\n",
            "  • ETN+JKB: 0.0000\n",
            "  • ETN+JKB+JX: 0.0000\n",
            "  • ETN+VCP+EC: 0.0000\n",
            "  • ETN+VCP+EF: 0.0000\n",
            "  • IC+ETN: 0.0000\n",
            "  • IC+JX: 0.0000\n",
            "  • JC+JX: 0.0000\n",
            "  • JKB+JKO: 0.0000\n",
            "  • JKB+JX+JKS: 0.0000\n",
            "  • JKB+VCP+EC: 0.0000\n",
            "  • JKG+NNG+VCP: 0.0000\n",
            "  • JKQ+JX: 0.0000\n",
            "  • JKS+EF: 0.0000\n",
            "  • JKS+ETN+JX: 0.0000\n",
            "  • JKS+JX: 0.0000\n",
            "  • JX+ETM: 0.0000\n",
            "  • JX+VCP+ETM: 0.0000\n",
            "  • MAG+VCP: 0.0000\n",
            "  • MAG+VCP+EC: 0.0000\n",
            "  • MAG+VCP+EF: 0.0000\n",
            "  • MAG+VCP+ETM: 0.0000\n",
            "  • MAG+XSV+EC: 0.0000\n",
            "  • MAG+XSV+EP+EC: 0.0000\n",
            "  • MAG+XSV+EP+ETM: 0.0000\n",
            "  • MAJ+JX: 0.0000\n",
            "  • MM+NNB+JX: 0.0000\n",
            "  • NA+EF: 0.0000\n",
            "  • NA+EP: 0.0000\n",
            "  • NA+ETM: 0.0000\n",
            "  • NNB+EC: 0.0000\n",
            "  • NNB+EF: 0.0000\n",
            "  • NNB+ETM: 0.0000\n",
            "  • NNB+JKB+JX: 0.0000\n",
            "  • NNB+JKO+JX: 0.0000\n",
            "  • NNB+VCP+EC+VCP: 0.0000\n",
            "  • NNB+VCP+ETM: 0.0000\n",
            "  • NNB+VCP+ETM+NNB+JX: 0.0000\n",
            "  • NNBC+JKO: 0.0000\n",
            "  • NNBC+JX: 0.0000\n",
            "  • NNBC+VCP+EC: 0.0000\n",
            "  • NNBC+VCP+EF: 0.0000\n",
            "  • NNBC+VCP+ETM: 0.0000\n",
            "  • NNG+EC: 0.0000\n",
            "  • NNG+EP: 0.0000\n",
            "  • NNG+ETM: 0.0000\n",
            "  • NNG+ETN: 0.0000\n",
            "  • NNG+JC: 0.0000\n",
            "  • NNG+JKB: 0.0000\n",
            "  • NNG+JKB+JX: 0.0000\n",
            "  • NNG+JKC: 0.0000\n",
            "  • NNG+JKO: 0.0000\n",
            "  • NNG+JKS: 0.0000\n",
            "  • NNG+JKV: 0.0000\n",
            "  • NNG+NNB: 0.0000\n",
            "  • NNG+VCP: 0.0000\n",
            "  • NNG+VCP+EF: 0.0000\n",
            "  • NNG+VCP+ETM: 0.0000\n",
            "  • NNG+VCP+JX: 0.0000\n",
            "  • NNG+XSV: 0.0000\n",
            "  • NNG+XSV+EC: 0.0000\n",
            "  • NNG+XSV+ETM: 0.0000\n",
            "  • NNP+EC: 0.0000\n",
            "  • NNP+ETM: 0.0000\n",
            "  • NNP+JKO: 0.0000\n",
            "  • NNP+JX: 0.0000\n",
            "  • NP+EP: 0.0000\n",
            "  • NP+JKB+JKB: 0.0000\n",
            "  • NP+JKB+JX: 0.0000\n",
            "  • NP+JKC: 0.0000\n",
            "  • NP+JKG+NNB: 0.0000\n",
            "  • NP+JKG+NNB+JKO: 0.0000\n",
            "  • NP+JKG+NNG+JKB: 0.0000\n",
            "  • NP+JKG+NNG+JKB+JX: 0.0000\n",
            "  • NP+JKG+NNG+JKG: 0.0000\n",
            "  • NP+JKG+NNG+JKO: 0.0000\n",
            "  • NP+JKG+NNG+VCP: 0.0000\n",
            "  • NP+JKO+VV+EC: 0.0000\n",
            "  • NP+NNG+XSN: 0.0000\n",
            "  • NP+VCP: 0.0000\n",
            "  • NP+VCP+EC+JKO: 0.0000\n",
            "  • NP+VCP+EC+JX: 0.0000\n",
            "  • NP+VCP+EF+JKO: 0.0000\n",
            "  • NP+VCP+EF+VCP: 0.0000\n",
            "  • NP+VCP+EP: 0.0000\n",
            "  • NP+VCP+ETM: 0.0000\n",
            "  • NP+VV+EC: 0.0000\n",
            "  • NR+JKO: 0.0000\n",
            "  • NR+JKS: 0.0000\n",
            "  • NR+JX: 0.0000\n",
            "  • NR+VCP: 0.0000\n",
            "  • NR+VCP+EC: 0.0000\n",
            "  • NR+VCP+EF: 0.0000\n",
            "  • NR+VCP+ETM: 0.0000\n",
            "  • UNA: 0.0000\n",
            "  • VA+EC+JX: 0.0000\n",
            "  • VA+EC+VCP: 0.0000\n",
            "  • VA+EC+VCP+EC: 0.0000\n",
            "  • VA+EC+VCP+EF: 0.0000\n",
            "  • VA+EC+VV+ETM: 0.0000\n",
            "  • VA+EC+VX: 0.0000\n",
            "  • VA+EC+VX+EC: 0.0000\n",
            "  • VA+EC+VX+EC+EC: 0.0000\n",
            "  • VA+EC+VX+EF: 0.0000\n",
            "  • VA+EC+VX+EP: 0.0000\n",
            "  • VA+EC+VX+ETM: 0.0000\n",
            "  • VA+EC+VX+ETN: 0.0000\n",
            "  • VA+EP+EC: 0.0000\n",
            "  • VA+EP+EF: 0.0000\n",
            "  • VA+EP+ETM: 0.0000\n",
            "  • VA+ETM+NNB: 0.0000\n",
            "  • VA+ETM+NNB+JKB+JX: 0.0000\n",
            "  • VA+ETM+NNB+JKS: 0.0000\n",
            "  • VA+ETM+NNB+XSA+EC: 0.0000\n",
            "  • VA+ETM+NNG+JKB: 0.0000\n",
            "  • VA+ETN+JKB: 0.0000\n",
            "  • VA+ETN+JKB+JX: 0.0000\n",
            "  • VA+ETN+JX: 0.0000\n",
            "  • VA+ETN+MAG: 0.0000\n",
            "  • VA+ETN+VCN+ETM: 0.0000\n",
            "  • VA+JX: 0.0000\n",
            "  • VCN+EC+VCP: 0.0000\n",
            "  • VCN+EF+VCP: 0.0000\n",
            "  • VCN+EP: 0.0000\n",
            "  • VCN+ETM+NNB+JKC: 0.0000\n",
            "  • VCP+EC+ETM: 0.0000\n",
            "  • VCP+EC+JX: 0.0000\n",
            "  • VCP+EC+VCP: 0.0000\n",
            "  • VCP+EC+VCP+EC: 0.0000\n",
            "  • VCP+EC+VV+EP+ETM: 0.0000\n",
            "  • VCP+EC+VX+EC: 0.0000\n",
            "  • VCP+EC+VX+ETM: 0.0000\n",
            "  • VCP+EF+JKQ: 0.0000\n",
            "  • VCP+EF+VCP: 0.0000\n",
            "  • VCP+EF+VX+ETM: 0.0000\n",
            "  • VCP+EP+EC: 0.0000\n",
            "  • VCP+EP+EF: 0.0000\n",
            "  • VCP+EP+EP: 0.0000\n",
            "  • VCP+EP+ETM: 0.0000\n",
            "  • VCP+EP+ETN+JKB: 0.0000\n",
            "  • VCP+EP+ETN+JKO: 0.0000\n",
            "  • VCP+ETM+NNB+JKC: 0.0000\n",
            "  • VCP+ETM+NNB+JKS: 0.0000\n",
            "  • VCP+ETM+NNB+JX: 0.0000\n",
            "  • VCP+ETM+NNB+VCP: 0.0000\n",
            "  • VCP+ETM+NNB+VCP+EF: 0.0000\n",
            "  • VCP+ETM+NNG+JX: 0.0000\n",
            "  • VCP+ETN+JKB+JX: 0.0000\n",
            "  • VCP+ETN+JX: 0.0000\n",
            "  • VCP+ETN+VCP+EC: 0.0000\n",
            "  • VCP+NA: 0.0000\n",
            "  • VCP+UNA: 0.0000\n",
            "  • VCP+VV: 0.0000\n",
            "  • VV+EC+EC: 0.0000\n",
            "  • VV+EC+EF: 0.0000\n",
            "  • VV+EC+EP: 0.0000\n",
            "  • VV+EC+JX: 0.0000\n",
            "  • VV+EC+VCP: 0.0000\n",
            "  • VV+EC+VCP+EC: 0.0000\n",
            "  • VV+EC+VV: 0.0000\n",
            "  • VV+EC+VV+EC: 0.0000\n",
            "  • VV+EC+VV+EF: 0.0000\n",
            "  • VV+EC+VV+EP: 0.0000\n",
            "  • VV+EC+VV+ETM: 0.0000\n",
            "  • VV+EC+VX+EC+VX: 0.0000\n",
            "  • VV+EC+VX+EC+VX+EF: 0.0000\n",
            "  • VV+EC+VX+EC+VX+EP: 0.0000\n",
            "  • VV+EC+VX+EP+EC: 0.0000\n",
            "  • VV+EC+VX+EP+EF: 0.0000\n",
            "  • VV+EC+VX+JX: 0.0000\n",
            "  • VV+EF+VCP: 0.0000\n",
            "  • VV+EP+EC+JX: 0.0000\n",
            "  • VV+EP+EC+VV+EP+EC: 0.0000\n",
            "  • VV+EP+EC+VX+EC: 0.0000\n",
            "  • VV+EP+EF: 0.0000\n",
            "  • VV+EP+EP: 0.0000\n",
            "  • VV+EP+EP+EC: 0.0000\n",
            "  • VV+EP+EP+ETM: 0.0000\n",
            "  • VV+EP+ETM: 0.0000\n",
            "  • VV+EP+ETN: 0.0000\n",
            "  • VV+ETM+EC: 0.0000\n",
            "  • VV+ETM+ETM: 0.0000\n",
            "  • VV+ETM+NNB: 0.0000\n",
            "  • VV+ETM+NNB+JKC: 0.0000\n",
            "  • VV+ETM+NNB+JKO: 0.0000\n",
            "  • VV+ETM+NNB+JKS: 0.0000\n",
            "  • VV+ETM+NNB+VCP: 0.0000\n",
            "  • VV+ETM+NNB+VCP+EF: 0.0000\n",
            "  • VV+ETM+NNG+JX: 0.0000\n",
            "  • VV+ETN+JKB: 0.0000\n",
            "  • VV+ETN+JKB+JX: 0.0000\n",
            "  • VV+ETN+JKS: 0.0000\n",
            "  • VV+ETN+VCP+EC: 0.0000\n",
            "  • VV+ETN+VCP+ETM: 0.0000\n",
            "  • VV+ETN+XSV+ETM: 0.0000\n",
            "  • VV+IC: 0.0000\n",
            "  • VV+JKO: 0.0000\n",
            "  • VV+JX: 0.0000\n",
            "  • VV+VCP: 0.0000\n",
            "  • VV+VV: 0.0000\n",
            "  • VX+EC+JKO: 0.0000\n",
            "  • VX+EC+JX: 0.0000\n",
            "  • VX+EC+VCP: 0.0000\n",
            "  • VX+EC+VCP+EC: 0.0000\n",
            "  • VX+EC+VV: 0.0000\n",
            "  • VX+EC+VV+EC: 0.0000\n",
            "  • VX+EC+VV+EF: 0.0000\n",
            "  • VX+EC+VX: 0.0000\n",
            "  • VX+EC+VX+EC: 0.0000\n",
            "  • VX+EC+VX+EF: 0.0000\n",
            "  • VX+EC+VX+EP: 0.0000\n",
            "  • VX+EC+VX+EP+EF: 0.0000\n",
            "  • VX+EC+VX+ETN: 0.0000\n",
            "  • VX+EP+EC+VX+EF: 0.0000\n",
            "  • VX+EP+EP: 0.0000\n",
            "  • VX+EP+ETM: 0.0000\n",
            "  • VX+EP+ETN+JX: 0.0000\n",
            "  • VX+ETM+NNB+ETM: 0.0000\n",
            "  • VX+ETM+NNB+JKC: 0.0000\n",
            "  • VX+ETM+NNB+JKO: 0.0000\n",
            "  • VX+ETM+NNB+JKS: 0.0000\n",
            "  • VX+ETM+NNB+JX: 0.0000\n",
            "  • VX+ETM+NNB+VCP: 0.0000\n",
            "  • VX+ETM+NNB+VCP+EC: 0.0000\n",
            "  • VX+ETM+NNB+VCP+EF: 0.0000\n",
            "  • VX+JX: 0.0000\n",
            "  • VX+UNA: 0.0000\n",
            "  • XPN+JX: 0.0000\n",
            "  • XR+JX: 0.0000\n",
            "  • XR+XSA+EC: 0.0000\n",
            "  • XSA+EC+JKO: 0.0000\n",
            "  • XSA+EC+JKS: 0.0000\n",
            "  • XSA+EC+JX: 0.0000\n",
            "  • XSA+EC+VCP: 0.0000\n",
            "  • XSA+EC+VCP+EC: 0.0000\n",
            "  • XSA+EC+VCP+ETM: 0.0000\n",
            "  • XSA+EC+VV+EF: 0.0000\n",
            "  • XSA+EC+VV+ETM: 0.0000\n",
            "  • XSA+EC+VX: 0.0000\n",
            "  • XSA+EC+VX+EC: 0.0000\n",
            "  • XSA+EC+VX+EC+VX: 0.0000\n",
            "  • XSA+EC+VX+EC+VX+EP: 0.0000\n",
            "  • XSA+EC+VX+EC+VX+ETM: 0.0000\n",
            "  • XSA+EC+VX+EF: 0.0000\n",
            "  • XSA+EC+VX+EP: 0.0000\n",
            "  • XSA+EC+VX+EP+EC: 0.0000\n",
            "  • XSA+EC+VX+ETN: 0.0000\n",
            "  • XSA+EP+EC+VX+EF: 0.0000\n",
            "  • XSA+EP+ETM: 0.0000\n",
            "  • XSA+EP+ETN+JKO: 0.0000\n",
            "  • XSA+ETM+NNB+VCP: 0.0000\n",
            "  • XSA+ETM+NNB+VCP+EF: 0.0000\n",
            "  • XSA+ETN+JKB: 0.0000\n",
            "  • XSA+ETN+JX: 0.0000\n",
            "  • XSA+ETN+VCP+EF: 0.0000\n",
            "  • XSA+ETN+VCP+ETM: 0.0000\n",
            "  • XSA+JKO: 0.0000\n",
            "  • XSA+XSN: 0.0000\n",
            "  • XSN+JKO: 0.0000\n",
            "  • XSN+JKV: 0.0000\n",
            "  • XSN+JX: 0.0000\n",
            "  • XSN+VCP: 0.0000\n",
            "  • XSN+VCP+EC: 0.0000\n",
            "  • XSN+VCP+EF: 0.0000\n",
            "  • XSV+EC+JKO: 0.0000\n",
            "  • XSV+EC+JKS: 0.0000\n",
            "  • XSV+EC+JX: 0.0000\n",
            "  • XSV+EC+JX+JKS: 0.0000\n",
            "  • XSV+EC+VCP: 0.0000\n",
            "  • XSV+EC+VCP+EC: 0.0000\n",
            "  • XSV+EC+VCP+VCP: 0.0000\n",
            "  • XSV+EC+VV+ETM: 0.0000\n",
            "  • XSV+EC+VX: 0.0000\n",
            "  • XSV+EC+VX+EC: 0.0000\n",
            "  • XSV+EC+VX+EC+VX: 0.0000\n",
            "  • XSV+EC+VX+EF: 0.0000\n",
            "  • XSV+EC+VX+EP: 0.0000\n",
            "  • XSV+EC+VX+EP+ETM: 0.0000\n",
            "  • XSV+EC+VX+ETM: 0.0000\n",
            "  • XSV+EC+VX+ETN: 0.0000\n",
            "  • XSV+EP+EC+JX: 0.0000\n",
            "  • XSV+EP+EF: 0.0000\n",
            "  • XSV+EP+ETM: 0.0000\n",
            "  • XSV+ETM+NNB+JKS: 0.0000\n",
            "  • XSV+ETM+NNG+JX: 0.0000\n",
            "  • XSV+ETN+JKB: 0.0000\n",
            "  • XSV+ETN+JKO: 0.0000\n",
            "  • XSV+ETN+JX: 0.0000\n",
            "  • XSV+ETN+VCP+EC: 0.0000\n",
            "  • XSV+JKO: 0.0000\n",
            "\n",
            "- 상위 20 토큰:\n",
            "  • \".\" 124359개\n",
            "  • \"영화\" 55547개\n",
            "  • \"다\" 51482개\n",
            "  • \"고\" 44371개\n",
            "  • \"하\" 37633개\n",
            "  • \"이\" 33549개\n",
            "  • \"을\" 28076개\n",
            "  • \"는\" 25398개\n",
            "  • \"보\" 23078개\n",
            "  • \"게\" 21132개\n",
            "  • \",\" 19315개\n",
            "  • \"지\" 18412개\n",
            "  • \"!\" 16563개\n",
            "  • \"없\" 15957개\n",
            "  • \"들\" 15900개\n",
            "  • \"있\" 15835개\n",
            "  • \"?\" 13730개\n",
            "  • \"좋\" 13422개\n",
            "  • \"나\" 12506개\n",
            "  • \"은\" 11988개\n",
            "\n",
            "- coverage 기준 vocab size 제안:\n",
            "  • 90% → 3065\n",
            "  • 95% → 7474\n",
            "  • 99% → 26416\n",
            "\n",
            "- 예시 문장 5개 토큰화:\n",
            "  1. 오늘 날씨가 정말 좋네요.\n",
            "     -> tokens: ['오늘', '날씨', '가', '정말', '좋', '네요', '.']\n",
            "     -> pos   : [('오늘', 'MAG'), ('날씨', 'NNG'), ('가', 'JKS'), ('정말', 'MAG'), ('좋', 'VA'), ('네요', 'EF'), ('.', 'SF')]\n",
            "  2. 저는 자연어 처리를 공부하고 있습니다.\n",
            "     -> tokens: ['저', '는', '자연어', '처리', '를', '공부', '하', '고', '있', '습니다', '.']\n",
            "     -> pos   : [('저', 'NP'), ('는', 'JX'), ('자연어', 'NNG'), ('처리', 'NNG'), ('를', 'JKO'), ('공부', 'NNG'), ('하', 'XSV'), ('고', 'EC'), ('있', 'VX'), ('습니다', 'EF'), ('.', 'SF')]\n",
            "  3. ELMo와 BERT는 문맥 기반 임베딩 모델입니다.\n",
            "     -> tokens: ['ELMo', '와', 'BERT', '는', '문맥', '기반', '임', '베', '딩', '모델', '입니다', '.']\n",
            "     -> pos   : [('ELMo', 'SL'), ('와', 'JC'), ('BERT', 'SL'), ('는', 'JX'), ('문맥', 'NNG'), ('기반', 'NNG'), ('임', 'VCP+ETN'), ('베', 'VV+EC'), ('딩', 'NNG'), ('모델', 'NNG'), ('입니다', 'VCP+EF'), ('.', 'SF')]\n",
            "  4. 파이썬으로 데이터를 전처리하고 시각화합니다.\n",
            "     -> tokens: ['파이썬', '으로', '데이터', '를', '전처리', '하', '고', '시각', '화', '합니다', '.']\n",
            "     -> pos   : [('파이썬', 'NNP'), ('으로', 'JKB'), ('데이터', 'NNG'), ('를', 'JKO'), ('전처리', 'NNG'), ('하', 'XSV'), ('고', 'EC'), ('시각', 'NNG'), ('화', 'XSN'), ('합니다', 'XSV+EF'), ('.', 'SF')]\n",
            "  5. 모델 성능을 높이기 위해 하이퍼파라미터를 조정합니다.\n",
            "     -> tokens: ['모델', '성능', '을', '높이', '기', '위해', '하이퍼', '파라미터', '를', '조정', '합니다', '.']\n",
            "     -> pos   : [('모델', 'NNG'), ('성능', 'NNG'), ('을', 'JKO'), ('높이', 'VV'), ('기', 'ETN'), ('위해', 'VV+EC'), ('하이퍼', 'NNG'), ('파라미터', 'NNG'), ('를', 'JKO'), ('조정', 'NNG'), ('합니다', 'XSV+EF'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unigram\n",
        "tensor_uni, wi_uni, iw_uni, uni_model_path, sp_uni = sentencepiece_unigram_tokenize(\n",
        "    corpus=filtered_corpus,\n",
        "    vocab_size=8000,\n",
        "    output_dir=\"./spm_models\",\n",
        "    user_defined_symbols=[\"<sep>\", \"<lab0>\", \"<lab1>\", \"<lab2>\"],\n",
        ")\n",
        "\n",
        "# Make a tokenize_one_fn bound to the trained model\n",
        "spm_uni_tokenize_one = spm_make_tokenize_one(uni_model_path)\n",
        "\n",
        "analyze_tokenizer(\n",
        "    corpus=filtered_corpus,\n",
        "    tokenize_one_fn=spm_uni_tokenize_one,\n",
        "    pos_pairs_one_fn=None,\n",
        "    example_sentences=example_sents,\n",
        "    title=\"SentencePiece Unigram Report\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn1MxJOco4af",
        "outputId": "7e2ee350-5e30-417f-edce-05ca09580229"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[SentencePiece Unigram Report] 요약\n",
            "- 전체 문장 수: 142673\n",
            "- 전체 토큰 수: 2118324\n",
            "- 문장당 평균 토큰 수: 14.85\n",
            "- 평균 토큰 '문자' 길이: 2.26\n",
            "\n",
            "- 상위 20 토큰:\n",
            "  • \"▁.\" 124359개\n",
            "  • \"▁\" 41345개\n",
            "  • \"▁영화\" 33856개\n",
            "  • \"▁,\" 19315개\n",
            "  • \"을\" 17031개\n",
            "  • \"▁!\" 16563개\n",
            "  • \"고\" 14754개\n",
            "  • \"▁?\" 13730개\n",
            "  • \"지\" 12046개\n",
            "  • \"다\" 11866개\n",
            "  • \"이\" 11257개\n",
            "  • \"▁너무\" 9976개\n",
            "  • \"만\" 9110개\n",
            "  • \"▁정말\" 8889개\n",
            "  • \"로\" 8771개\n",
            "  • \"▁진짜\" 7388개\n",
            "  • \"점\" 6637개\n",
            "  • \"기\" 6627개\n",
            "  • \"가\" 6490개\n",
            "  • \"나\" 6480개\n",
            "\n",
            "- coverage 기준 vocab size 제안:\n",
            "  • 90% → 3907\n",
            "  • 95% → 5360\n",
            "  • 99% → 7209\n",
            "\n",
            "- 예시 문장 5개 토큰화:\n",
            "  1. 오늘 날씨가 정말 좋네요.\n",
            "     -> ['▁오늘', '▁날', '씨', '가', '▁정말', '▁좋네요', '.']\n",
            "  2. 저는 자연어 처리를 공부하고 있습니다.\n",
            "     -> ['▁저는', '▁자연', '어', '▁', '처리', '를', '▁공부', '하고', '▁있습니다', '.']\n",
            "  3. ELMo와 BERT는 문맥 기반 임베딩 모델입니다.\n",
            "     -> ['▁', 'ELM', 'o', '와', '▁', 'BERT', '는', '▁문', '맥', '▁기', '반', '▁임', '베', '딩', '▁', '모델', '입니다', '.']\n",
            "  4. 파이썬으로 데이터를 전처리하고 시각화합니다.\n",
            "     -> ['▁파이', '썬', '으로', '▁데', '이', '터', '를', '▁전', '처리', '하고', '▁시각', '화', '합니다', '.']\n",
            "  5. 모델 성능을 높이기 위해 하이퍼파라미터를 조정합니다.\n",
            "     -> ['▁', '모델', '▁성', '능', '을', '▁높', '이기', '▁위해', '▁하', '이', '퍼', '파', '라', '미', '터', '를', '▁조', '정', '합니다', '.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'total_tokens': 2118324,\n",
              " 'avg_tokens_per_sentence': 14.847406306729374,\n",
              " 'avg_token_char_length': 2.26325481843193,\n",
              " 'pos_ratio': None,\n",
              " 'top_tokens': [('▁.', 124359),\n",
              "  ('▁', 41345),\n",
              "  ('▁영화', 33856),\n",
              "  ('▁,', 19315),\n",
              "  ('을', 17031),\n",
              "  ('▁!', 16563),\n",
              "  ('고', 14754),\n",
              "  ('▁?', 13730),\n",
              "  ('지', 12046),\n",
              "  ('다', 11866),\n",
              "  ('이', 11257),\n",
              "  ('▁너무', 9976),\n",
              "  ('만', 9110),\n",
              "  ('▁정말', 8889),\n",
              "  ('로', 8771),\n",
              "  ('▁진짜', 7388),\n",
              "  ('점', 6637),\n",
              "  ('기', 6627),\n",
              "  ('가', 6490),\n",
              "  ('나', 6480)],\n",
              " 'suggested_vocab_size': {0.9: 3907, 0.95: 5360, 0.99: 7209},\n",
              " 'examples': [('오늘 날씨가 정말 좋네요.',\n",
              "   ['▁오늘', '▁날', '씨', '가', '▁정말', '▁좋네요', '.'],\n",
              "   None),\n",
              "  ('저는 자연어 처리를 공부하고 있습니다.',\n",
              "   ['▁저는', '▁자연', '어', '▁', '처리', '를', '▁공부', '하고', '▁있습니다', '.'],\n",
              "   None),\n",
              "  ('ELMo와 BERT는 문맥 기반 임베딩 모델입니다.',\n",
              "   ['▁',\n",
              "    'ELM',\n",
              "    'o',\n",
              "    '와',\n",
              "    '▁',\n",
              "    'BERT',\n",
              "    '는',\n",
              "    '▁문',\n",
              "    '맥',\n",
              "    '▁기',\n",
              "    '반',\n",
              "    '▁임',\n",
              "    '베',\n",
              "    '딩',\n",
              "    '▁',\n",
              "    '모델',\n",
              "    '입니다',\n",
              "    '.'],\n",
              "   None),\n",
              "  ('파이썬으로 데이터를 전처리하고 시각화합니다.',\n",
              "   ['▁파이',\n",
              "    '썬',\n",
              "    '으로',\n",
              "    '▁데',\n",
              "    '이',\n",
              "    '터',\n",
              "    '를',\n",
              "    '▁전',\n",
              "    '처리',\n",
              "    '하고',\n",
              "    '▁시각',\n",
              "    '화',\n",
              "    '합니다',\n",
              "    '.'],\n",
              "   None),\n",
              "  ('모델 성능을 높이기 위해 하이퍼파라미터를 조정합니다.',\n",
              "   ['▁',\n",
              "    '모델',\n",
              "    '▁성',\n",
              "    '능',\n",
              "    '을',\n",
              "    '▁높',\n",
              "    '이기',\n",
              "    '▁위해',\n",
              "    '▁하',\n",
              "    '이',\n",
              "    '퍼',\n",
              "    '파',\n",
              "    '라',\n",
              "    '미',\n",
              "    '터',\n",
              "    '를',\n",
              "    '▁조',\n",
              "    '정',\n",
              "    '합니다',\n",
              "    '.'],\n",
              "   None)]}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BPE (다른 prefix로 자동 저장)\n",
        "tensor_bpe, wi_bpe, iw_bpe, bpe_model_path, sp_bpe = sentencepiece_bpe_tokenize(\n",
        "    corpus=filtered_corpus,\n",
        "    vocab_size=8000,\n",
        "    output_dir=\"./spm_models\",\n",
        "    user_defined_symbols=[\"<sep>\", \"<lab0>\", \"<lab1>\", \"<lab2>\"],\n",
        ")\n",
        "\n",
        "spm_bpe_tokenize_one = spm_make_tokenize_one(bpe_model_path)\n",
        "\n",
        "analyze_tokenizer(\n",
        "    corpus=filtered_corpus,\n",
        "    tokenize_one_fn=spm_bpe_tokenize_one,\n",
        "    pos_pairs_one_fn=None,\n",
        "    example_sentences=example_sents,\n",
        "    title=\"SentencePiece BPE Report\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2bfr7Aco4dm",
        "outputId": "5ebeb1e8-98fd-49b2-e125-5531e68394b5"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[SentencePiece BPE Report] 요약\n",
            "- 전체 문장 수: 142673\n",
            "- 전체 토큰 수: 2099557\n",
            "- 문장당 평균 토큰 수: 14.72\n",
            "- 평균 토큰 '문자' 길이: 2.28\n",
            "\n",
            "- 상위 20 토큰:\n",
            "  • \"▁.\" 124359개\n",
            "  • \"▁영화\" 30848개\n",
            "  • \"▁,\" 19315개\n",
            "  • \"▁!\" 16563개\n",
            "  • \"▁?\" 13730개\n",
            "  • \"을\" 11501개\n",
            "  • \"▁\" 11158개\n",
            "  • \"▁너무\" 9825개\n",
            "  • \"▁정말\" 8840개\n",
            "  • \"지\" 7927개\n",
            "  • \"다\" 7466개\n",
            "  • \"이\" 7374개\n",
            "  • \"▁진짜\" 7308개\n",
            "  • \"만\" 7155개\n",
            "  • \"고\" 6429개\n",
            "  • \"점\" 6020개\n",
            "  • \"로\" 5938개\n",
            "  • \"기\" 5566개\n",
            "  • \"▁연기\" 5444개\n",
            "  • \"나\" 5279개\n",
            "\n",
            "- coverage 기준 vocab size 제안:\n",
            "  • 90% → 4068\n",
            "  • 95% → 5503\n",
            "  • 99% → 7102\n",
            "\n",
            "- 예시 문장 5개 토큰화:\n",
            "  1. 오늘 날씨가 정말 좋네요.\n",
            "     -> ['▁오늘', '▁날', '씨', '가', '▁정말', '▁좋네요', '.']\n",
            "  2. 저는 자연어 처리를 공부하고 있습니다.\n",
            "     -> ['▁저는', '▁자연', '어', '▁처', '리', '를', '▁공부', '하고', '▁있습니다', '.']\n",
            "  3. ELMo와 BERT는 문맥 기반 임베딩 모델입니다.\n",
            "     -> ['▁', 'ELM', 'o', '와', '▁', 'BERT', '는', '▁문', '맥', '▁기', '반', '▁임', '베', '딩', '▁모', '델', '입니다', '.']\n",
            "  4. 파이썬으로 데이터를 전처리하고 시각화합니다.\n",
            "     -> ['▁파', '이', '썬', '으로', '▁데', '이터', '를', '▁전', '처', '리', '하고', '▁시각', '화', '합니다', '.']\n",
            "  5. 모델 성능을 높이기 위해 하이퍼파라미터를 조정합니다.\n",
            "     -> ['▁모', '델', '▁성', '능', '을', '▁높', '이기', '▁위해', '▁하이', '퍼', '파', '라', '미', '터', '를', '▁조', '정', '합니다', '.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'total_tokens': 2099557,\n",
              " 'avg_tokens_per_sentence': 14.715867753534306,\n",
              " 'avg_token_char_length': 2.283485039939378,\n",
              " 'pos_ratio': None,\n",
              " 'top_tokens': [('▁.', 124359),\n",
              "  ('▁영화', 30848),\n",
              "  ('▁,', 19315),\n",
              "  ('▁!', 16563),\n",
              "  ('▁?', 13730),\n",
              "  ('을', 11501),\n",
              "  ('▁', 11158),\n",
              "  ('▁너무', 9825),\n",
              "  ('▁정말', 8840),\n",
              "  ('지', 7927),\n",
              "  ('다', 7466),\n",
              "  ('이', 7374),\n",
              "  ('▁진짜', 7308),\n",
              "  ('만', 7155),\n",
              "  ('고', 6429),\n",
              "  ('점', 6020),\n",
              "  ('로', 5938),\n",
              "  ('기', 5566),\n",
              "  ('▁연기', 5444),\n",
              "  ('나', 5279)],\n",
              " 'suggested_vocab_size': {0.9: 4068, 0.95: 5503, 0.99: 7102},\n",
              " 'examples': [('오늘 날씨가 정말 좋네요.',\n",
              "   ['▁오늘', '▁날', '씨', '가', '▁정말', '▁좋네요', '.'],\n",
              "   None),\n",
              "  ('저는 자연어 처리를 공부하고 있습니다.',\n",
              "   ['▁저는', '▁자연', '어', '▁처', '리', '를', '▁공부', '하고', '▁있습니다', '.'],\n",
              "   None),\n",
              "  ('ELMo와 BERT는 문맥 기반 임베딩 모델입니다.',\n",
              "   ['▁',\n",
              "    'ELM',\n",
              "    'o',\n",
              "    '와',\n",
              "    '▁',\n",
              "    'BERT',\n",
              "    '는',\n",
              "    '▁문',\n",
              "    '맥',\n",
              "    '▁기',\n",
              "    '반',\n",
              "    '▁임',\n",
              "    '베',\n",
              "    '딩',\n",
              "    '▁모',\n",
              "    '델',\n",
              "    '입니다',\n",
              "    '.'],\n",
              "   None),\n",
              "  ('파이썬으로 데이터를 전처리하고 시각화합니다.',\n",
              "   ['▁파',\n",
              "    '이',\n",
              "    '썬',\n",
              "    '으로',\n",
              "    '▁데',\n",
              "    '이터',\n",
              "    '를',\n",
              "    '▁전',\n",
              "    '처',\n",
              "    '리',\n",
              "    '하고',\n",
              "    '▁시각',\n",
              "    '화',\n",
              "    '합니다',\n",
              "    '.'],\n",
              "   None),\n",
              "  ('모델 성능을 높이기 위해 하이퍼파라미터를 조정합니다.',\n",
              "   ['▁모',\n",
              "    '델',\n",
              "    '▁성',\n",
              "    '능',\n",
              "    '을',\n",
              "    '▁높',\n",
              "    '이기',\n",
              "    '▁위해',\n",
              "    '▁하이',\n",
              "    '퍼',\n",
              "    '파',\n",
              "    '라',\n",
              "    '미',\n",
              "    '터',\n",
              "    '를',\n",
              "    '▁조',\n",
              "    '정',\n",
              "    '합니다',\n",
              "    '.'],\n",
              "   None)]}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stats_okt = analyze_tokenizer(\n",
        "    corpus=filtered_corpus,\n",
        "    tokenize_one_fn=okt_tokenize_one,\n",
        "    pos_pairs_one_fn=okt_pos_pairs_one,\n",
        "    example_sentences=example_sents,\n",
        "    title=\"OKT Report\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogRIO9Seo4TN",
        "outputId": "aa250518-3341-4acd-a038-85325f130dc9"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[OKT Report] 요약\n",
            "- 전체 문장 수: 142673\n",
            "- 전체 토큰 수: 1798553\n",
            "- 문장당 평균 토큰 수: 12.61\n",
            "- 평균 토큰 '문자' 길이: 1.92\n",
            "- 품사 비율:\n",
            "  • Noun: 0.4714\n",
            "  • Verb: 0.1339\n",
            "  • Punctuation: 0.0967\n",
            "  • Josa: 0.0966\n",
            "  • Adjective: 0.0949\n",
            "  • Suffix: 0.0231\n",
            "  • Adverb: 0.0226\n",
            "  • Modifier: 0.0157\n",
            "  • Number: 0.0151\n",
            "  • Determiner: 0.0102\n",
            "  • Alpha: 0.0068\n",
            "  • VerbPrefix: 0.0056\n",
            "  • Exclamation: 0.0033\n",
            "  • Conjunction: 0.0021\n",
            "  • Foreign: 0.0018\n",
            "  • Eomi: 0.0003\n",
            "  • PreEomi: 0.0000\n",
            "\n",
            "- 상위 20 토큰:\n",
            "  • \".\" 124359개\n",
            "  • \"영화\" 49054개\n",
            "  • \"보다\" 40616개\n",
            "  • \"하다\" 33960개\n",
            "  • \"을\" 21698개\n",
            "  • \"이\" 20097개\n",
            "  • \",\" 19315개\n",
            "  • \"!\" 16563개\n",
            "  • \"없다\" 15756개\n",
            "  • \"?\" 13730개\n",
            "  • \"들\" 12800개\n",
            "  • \"이다\" 12545개\n",
            "  • \"있다\" 11706개\n",
            "  • \"좋다\" 11634개\n",
            "  • \"너무\" 10853개\n",
            "  • \"재밌다\" 9744개\n",
            "  • \"다\" 9439개\n",
            "  • \"정말\" 9377개\n",
            "  • \"것\" 8736개\n",
            "  • \"만\" 8664개\n",
            "\n",
            "- coverage 기준 vocab size 제안:\n",
            "  • 90% → 3631\n",
            "  • 95% → 8082\n",
            "  • 99% → 26164\n",
            "\n",
            "- 예시 문장 5개 토큰화:\n",
            "  1. 오늘 날씨가 정말 좋네요.\n",
            "     -> tokens: ['오늘', '날씨', '가', '정말', '좋다', '.']\n",
            "     -> pos   : [('오늘', 'Noun'), ('날씨', 'Noun'), ('가', 'Josa'), ('정말', 'Noun'), ('좋다', 'Adjective'), ('.', 'Punctuation')]\n",
            "  2. 저는 자연어 처리를 공부하고 있습니다.\n",
            "     -> tokens: ['저', '는', '자연어', '처리', '를', '공부', '하고', '있다', '.']\n",
            "     -> pos   : [('저', 'Noun'), ('는', 'Josa'), ('자연어', 'Noun'), ('처리', 'Noun'), ('를', 'Josa'), ('공부', 'Noun'), ('하고', 'Josa'), ('있다', 'Adjective'), ('.', 'Punctuation')]\n",
            "  3. ELMo와 BERT는 문맥 기반 임베딩 모델입니다.\n",
            "     -> tokens: ['ELMo', '오다', 'BERT', '늘다', '문맥', '기반', '임베딩', '모델', '이다', '.']\n",
            "     -> pos   : [('ELMo', 'Alpha'), ('오다', 'Verb'), ('BERT', 'Alpha'), ('늘다', 'Verb'), ('문맥', 'Noun'), ('기반', 'Noun'), ('임베딩', 'Noun'), ('모델', 'Noun'), ('이다', 'Adjective'), ('.', 'Punctuation')]\n",
            "  4. 파이썬으로 데이터를 전처리하고 시각화합니다.\n",
            "     -> tokens: ['파이썬', '으로', '데이터', '를', '전', '처리', '하고', '시각', '화하다', '.']\n",
            "     -> pos   : [('파이썬', 'Noun'), ('으로', 'Josa'), ('데이터', 'Noun'), ('를', 'Josa'), ('전', 'Modifier'), ('처리', 'Noun'), ('하고', 'Josa'), ('시각', 'Noun'), ('화하다', 'Adjective'), ('.', 'Punctuation')]\n",
            "  5. 모델 성능을 높이기 위해 하이퍼파라미터를 조정합니다.\n",
            "     -> tokens: ['모델', '성능', '을', '높이다', '위해', '하이퍼', '파라미터', '를', '조정', '하다', '.']\n",
            "     -> pos   : [('모델', 'Noun'), ('성능', 'Noun'), ('을', 'Josa'), ('높이다', 'Verb'), ('위해', 'Noun'), ('하이퍼', 'Noun'), ('파라미터', 'Noun'), ('를', 'Josa'), ('조정', 'Noun'), ('하다', 'Verb'), ('.', 'Punctuation')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_list = []"
      ],
      "metadata": {
        "id": "ZylxfsT-o4hF"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hUl6jE_Eo4lB"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM 모델 구성"
      ],
      "metadata": {
        "id": "jC6xvNU1GnrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "def create_dataloaders(train_tensor, train_labels, test_tensor, test_labels,\n",
        "                       batch_size=32, val_ratio=0.2):\n",
        "    dataset = TensorDataset(train_tensor, train_labels)\n",
        "    val_size = int(len(dataset) * val_ratio)\n",
        "    train_size = len(dataset) - val_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_dataset = TensorDataset(test_tensor, test_labels)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "itqg1eXYwU-X"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 후 문자열 corpus\n",
        "corpus = train_data['document'].tolist()\n",
        "labels = torch.tensor(train_data['label'].values, dtype=torch.long)  # 감정 레이블\n",
        "test_corpus = test_data['document'].tolist()\n",
        "test_labels = torch.tensor(test_data['label'].values, dtype=torch.long)\n",
        "\n",
        "def truncate_corpus(corpus, max_length=130):\n",
        "    \"\"\"\n",
        "    corpus: 전처리된 문장 리스트\n",
        "    max_length: 문장 최대 글자 수\n",
        "    \"\"\"\n",
        "    truncated_corpus = []\n",
        "    for sentence in corpus:\n",
        "        truncated_corpus.append(sentence[:max_length])\n",
        "    return truncated_corpus\n",
        "\n",
        "# 1. 문자열 최대 길이 제한\n",
        "corpus_truncated = filtered_corpus\n",
        "test_corp_turn = truncate_corpus(test_corpus)\n",
        "\n",
        "# 2. SentencePiece 모델 로드\n",
        "sp1 = spm.SentencePieceProcessor(model_file='./ko_unigram_8000.model')\n",
        "sp2 = spm.SentencePieceProcessor(model_file='./ko_bpe_8000.model')\n",
        "\n",
        "# 3. 토큰화 + 패딩\n",
        "tensor, word_index, index_word = sp_tokenize(sp1, corpus_truncated)\n",
        "test_tensor, test_word_index, test_index_word = sp_tokenize(sp1, test_corp_turn)\n",
        "\n",
        "\n",
        "# 4. DataLoader 생성\n",
        "train_loader, val_loader, test_loader = create_dataloaders(tensor, labels, test_tensor, test_labels)\n",
        "\n",
        "# 확인\n",
        "print(f\"Train batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\n",
        "for batch in train_loader:\n",
        "    x, y = batch\n",
        "    print(\"입력 텐서 shape:\", x.shape)\n",
        "    print(\"레이블 shape:\", y.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "U8bca13ZAtB2",
        "outputId": "c4b9e80d-efb1-42ae-8a6f-8c2f5d351e2b"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Not found: \"./ko_unigram_8000.model\": No such file or directory Error #2",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3288477328.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 2. SentencePiece 모델 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./ko_unigram_8000.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0msp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./ko_bpe_8000.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mInit\u001b[0;34m(self, model_file, model_proto, out_type, add_bos, add_eos, reverse, emit_unk_piece, enable_sampling, nbest_size, alpha, num_threads)\u001b[0m\n\u001b[1;32m    466\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmodel_file\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mLoad\u001b[0;34m(self, model_file, model_proto)\u001b[0m\n\u001b[1;32m    959\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromSerializedProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mLoadFromFile\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor_LoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_EncodeAsIds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_sampling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_bos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_eos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memit_unk_piece\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Not found: \"./ko_unigram_8000.model\": No such file or directory Error #2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 공통 하이퍼파라미터 정의\n",
        "VOCAB_SIZE = 8000                  # 단어 사전의 크기\n",
        "EMBEDDING_DIM = 100                # 임베딩 벡터의 차원\n",
        "HIDDEN_DIM = 128                   # LSTM의 은닉 상태 차원\n",
        "OUTPUT_DIM = 1                     # 출력 차원 (긍정=1, 부정=0 -> 1개)\n",
        "N_LAYERS = 2                       # LSTM 레이어 개수\n",
        "BIDIRECTIONAL = True               # 양방향 RNN/LSTM 여부\n",
        "DROPOUT_RATE = 0.2                 # 드롭아웃 비율\n",
        "PAD_IDX = 0                        # 패딩 인덱스 (0)"
      ],
      "metadata": {
        "id": "OSEdw0A7Atku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. 임베딩 레이어\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        # 2. LSTM 레이어\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
        "                           hidden_size=hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional,\n",
        "                           batch_first=True,\n",
        "                           dropout=dropout)\n",
        "\n",
        "        # 3. FC 레이어\n",
        "        fc_input_dim = hidden_dim * 2\n",
        "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
        "\n",
        "        # 4. 드롭아웃\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        # 2. LSTM\n",
        "        _output, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        # 3. 마지막 레이어의 은닉 상태 결합 (양방향 처리)\n",
        "        if self.lstm.bidirectional:\n",
        "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1,:,:])\n",
        "\n",
        "        # 4. FC 레이어 통과\n",
        "        prediction = self.fc(hidden)\n",
        "\n",
        "        return prediction.squeeze(1)"
      ],
      "metadata": {
        "id": "80DPiF8qAtny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "# 0. GPU 장치 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 1. 헬퍼 함수 정의\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "# 2. 훈련 함수 정의\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "    for texts, labels in iterator:\n",
        "        texts = texts.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(texts)\n",
        "        loss = criterion(predictions, labels.float())\n",
        "        acc = binary_accuracy(predictions, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # 클리핑\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "# 3. 평가 함수 정의\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in iterator:\n",
        "            texts = texts.to(device)\n",
        "            labels = labels.to(device)\n",
        "            predictions = model(texts)\n",
        "            loss = criterion(predictions, labels.float())\n",
        "            acc = binary_accuracy(predictions, labels)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "TSywKJilAtql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 설정\n",
        "lstm_model = LSTMModel(\n",
        "    VOCAB_SIZE,\n",
        "    EMBEDDING_DIM,\n",
        "    HIDDEN_DIM,\n",
        "    OUTPUT_DIM,\n",
        "    N_LAYERS,\n",
        "    BIDIRECTIONAL,\n",
        "    DROPOUT_RATE,\n",
        "    PAD_IDX\n",
        ").to(device)\n",
        "\n",
        "save_path = 'best_model_lstm.pt'\n",
        "N_EPOCHS = 20\n",
        "patience = 5\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, lstm_model.parameters()), lr=0.0001)\n",
        "\n",
        "# Early stopping 변수\n",
        "best_valid_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "train_losses, train_accs, valid_losses, valid_accs = [], [], [], []\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"--- LSTM Model Training starts ---\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# 학습 루프\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train(lstm_model, train_loader, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(lstm_model, val_loader, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "    # 기록\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    valid_losses.append(valid_loss)\n",
        "    valid_accs.append(valid_acc)\n",
        "\n",
        "    # Early Stopping\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(lstm_model.state_dict(), save_path)\n",
        "        patience_counter = 0\n",
        "        print(f'\\t>> Validation loss improved ({best_valid_loss:.3f}). Model saved.')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f'\\t>> Validation loss did not improve. Counter: {patience_counter}/{patience}')\n",
        "        if patience_counter >= patience:\n",
        "            print(f'--- Early stopping triggered after {epoch+1} epochs ---')\n",
        "            break\n",
        "\n",
        "# 테스트 평가\n",
        "print(f\"\\n--- Loading best LSTM model for test evaluation ---\")\n",
        "lstm_model.load_state_dict(torch.load(save_path))\n",
        "test_loss, test_acc = evaluate(lstm_model, test_loader, criterion)\n",
        "\n",
        "print(f\"\\n--- LSTM Model Test Results (Best Model) ---\")\n",
        "print(f'\\tTest Loss: {test_loss:.3f}')\n",
        "print(f'\\tTest Acc:  {test_acc*100:.2f}%')\n"
      ],
      "metadata": {
        "id": "HaYS6U3rAttd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DrCU_Q37Atvx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}